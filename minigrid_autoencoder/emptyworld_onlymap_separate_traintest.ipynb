{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9788dd5e",
   "metadata": {},
   "source": [
    "#### Imports and env creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96a09a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.2, Python 3.10.18)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import annotations\n",
    "\n",
    "from minigrid.core.constants import COLOR_NAMES\n",
    "from minigrid.core.grid import Grid\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Door, Goal, Key, Wall\n",
    "from minigrid.manual_control import ManualControl\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "from minigrid.core.constants import IDX_TO_OBJECT, OBJECT_TO_IDX\n",
    "import gymnasium as gym\n",
    "\n",
    "from minigrid.core.actions import Actions\n",
    "from dataclasses import dataclass, field \n",
    "from typing import Set\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2c267bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  2  2  2  2  2  2  2]\n",
      " [ 2 10  1  1  1  1  1  2]\n",
      " [ 2  1  1  1  6  1  1  2]\n",
      " [ 2  1  1  1  1  1  1  2]\n",
      " [ 2  1  1  6  1  1  1  2]\n",
      " [ 2  1  1  1  1  1  1  2]\n",
      " [ 2  1  1  6  1  6  8  2]\n",
      " [ 2  2  2  2  2  2  2  2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/tiago/Desktop/Code/SURFiN/mg10/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.agent_pos to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.agent_pos` for environment variables or `env.get_wrapper_attr('agent_pos')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "WORLD_N=8\n",
    "WORLD_N_SQ = WORLD_N**2\n",
    "\n",
    "# class SimpleEnv(MiniGridEnv):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         size=WORLD_N,\n",
    "#         agent_start_pos=(1, 1),\n",
    "#         agent_start_dir=0,\n",
    "#         max_steps: int | None = None,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         self.agent_start_pos = agent_start_pos\n",
    "#         self.agent_start_dir = agent_start_dir\n",
    "\n",
    "#         mission_space = MissionSpace(mission_func=self._gen_mission)\n",
    "\n",
    "#         super().__init__(\n",
    "#             mission_space=mission_space,\n",
    "#             grid_size=size,\n",
    "#             max_steps=256,\n",
    "#             **kwargs,\n",
    "#         )\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _gen_mission():\n",
    "#         return \"grand mission\"\n",
    "    \n",
    "    \n",
    "#     def _gen_grid(self, width, height):\n",
    "#       self.grid = Grid(width, height)\n",
    "#       self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "#       if self.agent_start_pos is not None:\n",
    "#           self.agent_pos = self.agent_start_pos\n",
    "#           self.agent_dir = self.agent_start_dir\n",
    "#       else:\n",
    "#           self.place_agent()\n",
    "#       self.valid_actions = {Actions.left, Actions.right, Actions.forward}\n",
    "    \n",
    "      \n",
    "#       self.put_obj(Goal(), width - 2, height - 2)\n",
    "      \n",
    "#       self.grid.set(3, 1, Wall())\n",
    "#       self.grid.set(3, 2, Wall())\n",
    "#       self.grid.set(3, 4, Wall())\n",
    "#       self.grid.set(3, 5, Wall())\n",
    "    \n",
    "#     def get_array_repr(self, with_agent=True):\n",
    "#         grid_array = self.unwrapped.grid.encode()[:,:,0]\n",
    "    \n",
    "#         grid_array[self.agent_pos[0],self.agent_pos[1]]=OBJECT_TO_IDX['agent']\n",
    "#         return grid_array.T\n",
    "    \n",
    "env = gym.make(\"MiniGrid-Dynamic-Obstacles-8x8-v0\", render_mode=\"rgb_array\")\n",
    "env.valid_actions = {Actions.left, Actions.right, Actions.forward}\n",
    "\n",
    "# env = SimpleEnv(render_mode=\"human\")\n",
    "# manual_control = ManualControl(env, seed=42)\n",
    "\n",
    "def get_array_repr(env):\n",
    "    grid_array = env.unwrapped.grid.encode()[:,:,0]\n",
    "    # print(grid_array)\n",
    "    # print(self.agent_pos)\n",
    "    grid_array[env.agent_pos[0],env.agent_pos[1]]=OBJECT_TO_IDX['agent']\n",
    "    return grid_array.T\n",
    "\n",
    "# env = SimpleEnv()\n",
    "env.reset()\n",
    "print(get_array_repr(env))\n",
    "\n",
    "# enable manual control for testing\n",
    "# manual_control.start()\n",
    "\n",
    "map_numbers = [1,2,3,4,5,6,7,8,9,10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b14348b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gymnasium.wrappers.order_enforcing.OrderEnforcing'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb6e19bd870>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(type(env))\n",
    "# env_core = env.unwrapped\n",
    "# plt.axis('off')\n",
    "# plt.imshow(env_core.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c468843",
   "metadata": {},
   "source": [
    "#### Agent motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83bc3d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/tiago/Desktop/Code/SURFiN/mg10/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.agent_dir to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.agent_dir` for environment variables or `env.get_wrapper_attr('agent_dir')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 2,  2,  2,  2,  2,  2,  2,  2,  2, 10,  1,  6,  1,  1,  1,  2,  2,\n",
      "        6,  1,  1,  1,  1,  1,  2,  2,  1,  1,  1,  1,  1,  1,  2,  2,  1,\n",
      "        1,  6,  1,  1,  1,  2,  2,  1,  1,  1,  6,  1,  1,  2,  2,  1,  1,\n",
      "        1,  1,  1,  8,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  0]), array([ 2,  2,  2,  2,  2,  2,  2,  2,  2, 10,  6,  1,  6,  1,  1,  2,  2,\n",
      "        1,  1,  1,  1,  1,  1,  2,  2,  1,  1,  1,  1,  1,  1,  2,  2,  1,\n",
      "        1,  1,  1,  1,  1,  2,  2,  1,  1,  6,  1,  1,  1,  2,  2,  1,  1,\n",
      "        6,  1,  1,  8,  2,  2,  2,  2,  2,  2,  2,  2,  2,  0,  0]), array([ 2,  2,  2,  2,  2,  2,  2,  2,  2, 10,  1,  6,  1,  1,  1,  2,  2,\n",
      "        1,  1,  6,  1,  1,  1,  2,  2,  1,  1,  1,  1,  1,  1,  2,  2,  1,\n",
      "        6,  1,  1,  1,  1,  2,  2,  1,  1,  1,  6,  1,  1,  2,  2,  1,  1,\n",
      "        1,  1,  1,  8,  2,  2,  2,  2,  2,  2,  2,  2,  2,  0,  3])]\n"
     ]
    }
   ],
   "source": [
    "from agents.random import RandomAgent\n",
    "n_steps_train = 8000\n",
    "n_steps_test = 2000\n",
    "\n",
    "random_action_agent = RandomAgent(valid_actions=env.valid_actions)\n",
    "\n",
    "dirarr = ['Right', 'Down', 'Left','Up']\n",
    "actions_to_idx = {Actions.left:0, Actions.right:1, Actions.forward:2}\n",
    "\n",
    "image_list_train = []\n",
    "\n",
    "for i in range(n_steps_train):\n",
    "    action = random_action_agent.act()\n",
    "    arr = get_array_repr(env).flatten()\n",
    "    arr = np.append(arr, [actions_to_idx[action], env.agent_dir])\n",
    "    image_list_train.append(arr)\n",
    "    # print(dirarr[env.agent_dir])\n",
    "    # print(action)\n",
    "    env.step(action)\n",
    "    # print(env.get_array_repr())\n",
    "\n",
    "env.reset()\n",
    "image_list_test = []\n",
    "\n",
    "for i in range(n_steps_test):\n",
    "    action = random_action_agent.act()\n",
    "    arr = get_array_repr(env).flatten()\n",
    "    arr = np.append(arr, [actions_to_idx[action], env.agent_dir])\n",
    "    image_list_test.append(arr)\n",
    "    # print(dirarr[env.agent_dir])\n",
    "    # print(action)\n",
    "    env.step(action)\n",
    "\n",
    "print(image_list_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b4188951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 2,  2,  2,  2,  2,  2,  2,  2, 10,  1,  2,  1,  1,  2,  2,  1,  1,\n",
      "        2,  1,  1,  2,  2,  1,  1,  1,  1,  1,  2,  2,  1,  1,  2,  1,  1,\n",
      "        2,  2,  1,  1,  2,  1,  8,  2,  2,  2,  2,  2,  2,  2,  2,  1,  0]), array([ 2,  2,  2,  2,  2,  2,  2,  2, 10,  1,  2,  1,  1,  2,  2,  1,  1,\n",
      "        2,  1,  1,  2,  2,  1,  1,  1,  1,  1,  2,  2,  1,  1,  2,  1,  1,\n",
      "        2,  2,  1,  1,  2,  1,  8,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1]), array([ 2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  2,  1,  1,  2,  2, 10,  1,\n",
      "        2,  1,  1,  2,  2,  1,  1,  1,  1,  1,  2,  2,  1,  1,  2,  1,  1,\n",
      "        2,  2,  1,  1,  2,  1,  8,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1])]\n",
      "[array([ 2,  2,  2,  2,  2,  2,  2,  2, 10,  1,  2,  1,  1,  2,  2,  1,  1,\n",
      "        2,  1,  1,  2,  2,  1,  1,  1,  1,  1,  2,  2,  1,  1,  2,  1,  1,\n",
      "        2,  2,  1,  1,  2,  1,  8,  2,  2,  2,  2,  2,  2,  2,  2,  0,  0]), array([ 2,  2,  2,  2,  2,  2,  2,  2, 10,  1,  2,  1,  1,  2,  2,  1,  1,\n",
      "        2,  1,  1,  2,  2,  1,  1,  1,  1,  1,  2,  2,  1,  1,  2,  1,  1,\n",
      "        2,  2,  1,  1,  2,  1,  8,  2,  2,  2,  2,  2,  2,  2,  2,  0,  3]), array([ 2,  2,  2,  2,  2,  2,  2,  2, 10,  1,  2,  1,  1,  2,  2,  1,  1,\n",
      "        2,  1,  1,  2,  2,  1,  1,  1,  1,  1,  2,  2,  1,  1,  2,  1,  1,\n",
      "        2,  2,  1,  1,  2,  1,  8,  2,  2,  2,  2,  2,  2,  2,  2,  1,  2])]\n"
     ]
    }
   ],
   "source": [
    "print(image_list_train[:3])\n",
    "print(image_list_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f2565",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fb66f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db192e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1+cu121\n",
      "True\n",
      "0\n",
      "NVIDIA GeForce MX330\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())  # This should return True if CUDA is available\n",
    "print(torch.cuda.current_device())  # Shows the current GPU device id (e.g., 0)\n",
    "print(torch.cuda.get_device_name(0))  # Displays the GPU name\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bdd3d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Modified) Neuromatch helper funcitons\n",
    "def init_weights_kaiming_normal(layer):\n",
    "  \"\"\"\n",
    "  Initializes weights from linear PyTorch layer\n",
    "  with kaiming normal distribution.\n",
    "\n",
    "  Args:\n",
    "    layer (torch.Module)\n",
    "        Pytorch layer\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  # check for linear PyTorch layer\n",
    "  if isinstance(layer, nn.Linear):\n",
    "    # initialize weights with kaiming normal distribution\n",
    "    nn.init.kaiming_normal_(layer.weight.data)\n",
    "    \n",
    "\n",
    "def runSGD(net, input_train, target_train, input_test, target_test, criterion='mse',\n",
    "           n_epochs=10, batch_size=32, verbose=False):\n",
    "  \"\"\"\n",
    "  Trains autoencoder network with stochastic gradient descent with Adam\n",
    "  optimizer and loss criterion. Train samples are shuffled, and loss is\n",
    "  displayed at the end of each opoch for both MSE and BCE. Plots training loss\n",
    "  at each minibatch (maximum of 500 randomly selected values).\n",
    "\n",
    "  Args:\n",
    "    net (torch network)\n",
    "        ANN object (nn.Module)\n",
    "\n",
    "    input_train (torch.Tensor)\n",
    "        vectorized input images from train set\n",
    "\n",
    "    input_test (torch.Tensor)\n",
    "        vectorized input images from test set\n",
    "\n",
    "    criterion (string)\n",
    "        train loss: 'bce' or 'mse'\n",
    "\n",
    "    n_epochs (boolean)\n",
    "        number of full iterations of training data\n",
    "\n",
    "    batch_size (integer)\n",
    "        number of element in mini-batches\n",
    "\n",
    "    verbose (boolean)\n",
    "        print final loss\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "\n",
    "  # 1. Define the device\n",
    "  # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  device = torch.device('cuda')\n",
    "  print(f\"Using device: {device}\")\n",
    "\n",
    "  # 2. Move the network to the device\n",
    "  net.to(device)\n",
    "\n",
    "  # 3. Move the main tensors to the device (crucial for initial setup)\n",
    "  input_train = input_train.to(device)\n",
    "  target_train = target_train.to(device)\n",
    "  input_test = input_test.to(device)\n",
    "  target_test = target_test.to(device)\n",
    "\n",
    "  # Initialize loss function\n",
    "  if criterion == 'mse':\n",
    "    loss_fn = nn.MSELoss()\n",
    "  elif criterion == 'bce':\n",
    "    loss_fn = nn.BCELoss()\n",
    "  elif criterion == 'cel':\n",
    "    loss_fn = nn.CrossEntropyLoss() \n",
    "  else:\n",
    "    print('Please specify either \"mse\" or \"bce\" for loss criterion')\n",
    "\n",
    "  # Move the loss function to the device if it has parameters (CrossEntropyLoss does not, \n",
    "  # but it's good practice for others like L1Loss which might have reduction='none')\n",
    "  loss_fn.to(device)\n",
    "\n",
    "  # Initialize SGD optimizer\n",
    "  optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "  # Placeholder for loss\n",
    "  track_loss = []\n",
    "\n",
    "  print('Epoch', '\\t', 'Loss train', '\\t', 'Loss test')\n",
    "  for i in range(n_epochs):\n",
    "\n",
    "    \n",
    "    shuffle_idx = np.random.permutation(len(input_train))\n",
    "\n",
    "    batches_input = torch.split(input_train[shuffle_idx], batch_size)\n",
    "    batches_target = torch.split(target_train[shuffle_idx], batch_size)\n",
    "\n",
    "    batches = zip(batches_input, batches_target)\n",
    "\n",
    "\n",
    "    shuffle_idx = np.random.permutation(len(input_train))\n",
    "    # batches = torch.split(input_train[shuffle_idx], batch_size)\n",
    "    # for batch in batches:\n",
    "    #   output_train = net(batch)\n",
    "    #   loss = loss_fn(output_train, batch)\n",
    "    for batch_input, batch_target in batches:\n",
    "      batch_input = batch_input.float()\n",
    "      batch_target = batch_target.float()\n",
    "      output_train = net(batch_input)  # Forward pass on the input batch\n",
    "      loss = loss_fn(output_train, batch_target)  # Compare output with the target\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # Keep track of loss at each epoch\n",
    "      track_loss += [float(loss)]\n",
    "\n",
    "    loss_epoch = f'{i+1}/{n_epochs}'\n",
    "    with torch.no_grad():\n",
    "      output_train = net(input_train)\n",
    "      loss_train = loss_fn(output_train, target_train)\n",
    "      loss_epoch += f'\\t {loss_train:.4f}'\n",
    "\n",
    "      output_test = net(input_test)\n",
    "      loss_test = loss_fn(output_test, target_test)\n",
    "      loss_epoch += f'\\t\\t {loss_test:.4f}'\n",
    "\n",
    "    print(loss_epoch)\n",
    "\n",
    "  if verbose:\n",
    "    # Print loss\n",
    "    loss_mse = f'\\nMSE\\t {eval_mse(output_train, target_train):0.4f}'\n",
    "    loss_mse += f'\\t\\t {eval_mse(output_test, target_test):0.4f}'\n",
    "    print(loss_mse)\n",
    "\n",
    "    loss_bce = f'BCE\\t {eval_bce(output_train, target_train):0.4f}'\n",
    "    loss_bce += f'\\t\\t {eval_bce(output_test, target_test):0.4f}'\n",
    "    print(loss_bce)\n",
    "\n",
    "  # Plot loss\n",
    "  step = int(np.ceil(len(track_loss) / 500))\n",
    "  x_range = np.arange(0, len(track_loss), step)\n",
    "  plt.figure()\n",
    "  plt.plot(x_range, track_loss[::step], 'C0')\n",
    "  plt.xlabel('Iterations')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlim([0, None])\n",
    "  plt.ylim([0, None])\n",
    "  plt.show()\n",
    "  print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c0d9a3",
   "metadata": {},
   "source": [
    "#### Input preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1276ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 2, 2,  ..., 2, 0, 0],\n",
      "        [2, 2, 2,  ..., 2, 0, 3],\n",
      "        [2, 2, 2,  ..., 2, 0, 2],\n",
      "        ...,\n",
      "        [2, 2, 2,  ..., 2, 0, 2],\n",
      "        [2, 2, 2,  ..., 2, 1, 1],\n",
      "        [2, 2, 2,  ..., 2, 1, 2]])\n",
      "tensor([[2, 2, 2,  ..., 2, 2, 0],\n",
      "        [2, 2, 2,  ..., 2, 0, 0],\n",
      "        [2, 2, 2,  ..., 2, 0, 3],\n",
      "        ...,\n",
      "        [2, 2, 2,  ..., 2, 0, 3],\n",
      "        [2, 2, 2,  ..., 2, 0, 2],\n",
      "        [2, 2, 2,  ..., 2, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# test_size=0.2\n",
    "# cutoff = int((1-test_size)*len(image_list))\n",
    "X_train, X_test = image_list_train, image_list_test \n",
    "\n",
    "target_train = X_train[1:]\n",
    "input_train = X_train[:-1]\n",
    "target_test = X_test[1:]\n",
    "input_test = X_test[:-1]\n",
    "\n",
    "target_train = torch.tensor(target_train, dtype=torch.int64)\n",
    "input_train = torch.tensor(input_train, dtype=torch.int64)\n",
    "target_test = torch.tensor(target_test, dtype=torch.int64)\n",
    "input_test = torch.tensor(input_test, dtype=torch.int64)\n",
    "\n",
    "\n",
    "def one_hot_encode_set(values, classes):\n",
    "    classdict = {}\n",
    "    for i in range(len(classes)):\n",
    "        classdict[classes[i]] = i\n",
    "    one_hot = torch.zeros(len(classes))\n",
    "    one_hot[classdict[values.item()]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def continuous_class(values, classes, direct):\n",
    "    classdict = {}\n",
    "    for i in range(len(classes)):\n",
    "        classdict[classes[i]] = i\n",
    "    val = classdict[values.item()]\n",
    "    if (values.item()==10):\n",
    "        val+=direct\n",
    "    return torch.tensor([val])\n",
    "\n",
    "\n",
    "# Define a function for one-hot encoding\n",
    "def one_hot_encode(values, num_classes):\n",
    "    one_hot = torch.zeros(num_classes)\n",
    "    one_hot[values] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Function to process the entire array (each input array of shape [length])\n",
    "def process_input(input_array, mode):\n",
    "    if mode == 'in':\n",
    "    \n",
    "        # first_part = torch.cat([one_hot_encode(input_array[i], 11) for i in range(25)])\n",
    "        # first_part = torch.cat([one_hot_encode_set(input_array[i], map_numbers) for i in range(25)])\n",
    "        first_part = torch.cat([continuous_class(input_array[i], map_numbers, input_array[1+WORLD_N_SQ]) for i in range(WORLD_N_SQ)])\n",
    "\n",
    "        # 26th position: one-hot encoded into 3 classes\n",
    "        second_part = one_hot_encode(input_array[WORLD_N_SQ], 3)\n",
    "        \n",
    "        # Last position: one-hot encoded into 4 classes\n",
    "        # third_part = one_hot_encode(input_array[26], 4)\n",
    "\n",
    "        # print(torch.cat([first_part, second_part, third_part], dim=0))\n",
    "        # Concatenate all parts to form the final one-hot encoded array\n",
    "        return torch.cat([first_part, second_part], dim=0)\n",
    "    elif mode == 'out':\n",
    "        return torch.cat([continuous_class(input_array[i], map_numbers, input_array[1+WORLD_N_SQ]) for i in range(WORLD_N_SQ)])\n",
    "\n",
    "\n",
    "# Apply to the whole dataset\n",
    "input_orientation_train = [x[1+WORLD_N_SQ] for x in input_train]\n",
    "input_orientation_test = [x[1+WORLD_N_SQ] for x in input_test]\n",
    "target_orientation_train = [x[1+WORLD_N_SQ] for x in target_train]\n",
    "target_orientation_test = [x[1+WORLD_N_SQ] for x in target_test]\n",
    "\n",
    "# Apply to the whole dataset\n",
    "input_train_processed = torch.stack([process_input(x, 'in') for x in input_train])\n",
    "input_test_processed = torch.stack([process_input(x, 'in') for x in input_test])\n",
    "target_train_processed = torch.stack([process_input(x, 'out') for x in target_train])\n",
    "target_test_processed = torch.stack([process_input(x, 'out') for x in target_test])\n",
    "\n",
    "\n",
    "print(target_train)\n",
    "print(input_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b84df5",
   "metadata": {},
   "source": [
    "#### Autoencoder definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "de53dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Module\n",
    "\n",
    "class RNNOutOnly(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(*args, **kwargs)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "43198093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder \n",
      "\n",
      " Sequential(\n",
      "  (0): Linear(in_features=67, out_features=1005, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1005, out_features=50, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "Latent \n",
      "\n",
      " Sequential(\n",
      "  (4): RNNOutOnly(\n",
      "    (rnn): RNN(50, 50)\n",
      "  )\n",
      ")\n",
      "\n",
      "Decoder \n",
      "\n",
      " Sequential(\n",
      "  (5): Linear(in_features=50, out_features=960, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): Linear(in_features=960, out_features=64, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoding_size=50\n",
    "hidden_fac=15\n",
    "input_size=input_train_processed.size(1)\n",
    "output_size = target_train_processed.size(1)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, int(input_size * hidden_fac)),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(int(input_size * hidden_fac), encoding_size),\n",
    "    nn.ReLU(),\n",
    "    RNNOutOnly(encoding_size, encoding_size,nonlinearity='relu'),\n",
    "    nn.Linear(encoding_size, int(output_size * hidden_fac)),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(int(output_size *hidden_fac), output_size),\n",
    "    )\n",
    "\n",
    "# model[:-2].apply(init_weights_kaiming_normal)\n",
    "\n",
    "n_e = 4\n",
    "n_l = 1\n",
    "encoder = model[:n_e]\n",
    "latent = model[n_e:n_e+n_l]\n",
    "decoder = model[n_e+n_l:]\n",
    "print(f'Encoder \\n\\n {encoder}\\n')\n",
    "print(f'Latent \\n\\n {latent}\\n')\n",
    "print(f'Decoder \\n\\n {decoder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da42b36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 20])\n",
      "torch.Size([1, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "# rnn = nn.RNN(input_size=10, hidden_size=20)  # 10 â†’ 20 features\n",
    "\n",
    "# x = torch.randn(5, 3, 10)  # (seq_len=5, batch=3, input_size=10)\n",
    "# output, hidden = rnn(x)\n",
    "\n",
    "# print(output.shape)  \n",
    "# print(hidden.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c672a46",
   "metadata": {},
   "source": [
    "#### Autoencoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "19c23a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch \t Loss train \t Loss test\n",
      "1/30\t 2.8113\t\t 2.8458\n",
      "2/30\t 2.5844\t\t 2.6569\n",
      "3/30\t 2.3964\t\t 2.4988\n",
      "4/30\t 2.3024\t\t 2.4189\n",
      "5/30\t 2.2119\t\t 2.3316\n",
      "6/30\t 2.1408\t\t 2.2712\n",
      "7/30\t 2.1016\t\t 2.2402\n",
      "8/30\t 2.0445\t\t 2.1883\n",
      "9/30\t 2.0153\t\t 2.1641\n",
      "10/30\t 1.9702\t\t 2.1315\n",
      "11/30\t 1.9356\t\t 2.1073\n",
      "12/30\t 1.8916\t\t 2.0670\n",
      "13/30\t 1.8591\t\t 2.0469\n",
      "14/30\t 1.8255\t\t 2.0223\n",
      "15/30\t 1.7978\t\t 2.0030\n",
      "16/30\t 1.7793\t\t 1.9841\n",
      "17/30\t 1.7343\t\t 1.9572\n",
      "18/30\t 1.7172\t\t 1.9620\n",
      "19/30\t 1.6915\t\t 1.9423\n",
      "20/30\t 1.6695\t\t 1.9330\n",
      "21/30\t 1.6389\t\t 1.9169\n",
      "22/30\t 1.6270\t\t 1.9072\n",
      "23/30\t 1.6115\t\t 1.9098\n",
      "24/30\t 1.5854\t\t 1.8942\n",
      "25/30\t 1.5757\t\t 1.9006\n",
      "26/30\t 1.5818\t\t 1.9078\n",
      "27/30\t 1.5436\t\t 1.8922\n",
      "28/30\t 1.5215\t\t 1.8794\n",
      "29/30\t 1.5051\t\t 1.8724\n",
      "30/30\t 1.4888\t\t 1.8781\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAGwCAYAAACkfh/eAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABPzklEQVR4nO3deVhUZf8G8HtmYIZ9FxBZXRERxB33FPdMs6zUzOytXksrs9X2zbR6Ky3NyhZbLFvUFjNNccEFEVBcQBEFBNn3fRlmzvvHwGHGQUVCBo7357q4fsw5Z2aemdMr9+9Zvo9MEAQBRERERBIiN3UDiIiIiFobAw4RERFJDgMOERERSQ4DDhEREUkOAw4RERFJDgMOERERSQ4DDhEREUmOmakb8G9otVpkZmbC1tYWMpnM1M0hIiKiZhAEAWVlZfDw8IBcfmP6Wjp0wMnMzISXl5epm0FEREQtkJ6eDk9Pzxvy2h064Nja2gLQfUF2dnYmbg0RERE1R2lpKby8vMS/4zdChw44DcNSdnZ2DDhEREQdzI2cXsJJxkRERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOZIIOBfyyk3dBCIiImpHJBFwqmrrTN0EIiIiakckEXDUGq2pm0BERETtiCQCTp3G1C0gIiKi9kQSAUetZQ8OERERNZJGwOEQFREREemRRMCprRNM3QQiIiJqRyQRcOrYg0NERER6JBFwOERFRERE+hhwiIiISHKkEXC0nINDREREjaQRcOrYg0NERESNpBFwOERFREREeiQScDhERURERI0YcIiIiEhyJBJwOERFREREjRhwiIiISHIkEXBYyZiIiIj0SSLg1HIODhEREemRRMDhEBURERHpY8AhIiIiyZFEwOEcHCIiItIniYDDOThERESkTxIBp07LHhwiIiJqJI2Awx4cIiIi0iOJgKOuY8AhIiKiRpIIOLWcZExERER6JBFw1JyDQ0RERHokEXC4TJyIiIj0SSLgqDnJmIiIiPRIIuDU1rEHh4iIiBpJIuBwiIqIiIj0SSLgcC8qIiIi0ieNgKPlHBwiIiJqJImAw72oiIiISF+7CTgrV66ETCbDkiVLrvu5dRotBIEhh4iIiHTaRcCJjo7GZ599hqCgoBY9XxAADYepiIiIqJ7JA055eTnmzp2L9evXw9HR8arX1tTUoLS01OCnAWvhEBERUQOTB5xFixZh6tSpCAsLu+a1K1asgL29vfjj5eUlnuN+VERERNTApAFn06ZNOHbsGFasWNGs65ctW4aSkhLxJz09XTzHpeJERETUwMxUb5yeno4nnngCu3btgoWFRbOeo1KpoFKpmjzHgENEREQNTBZwYmNjkZubi/79+4vHNBoNIiIisGbNGtTU1EChUDT79dR1nINDREREOiYLOOPGjcOpU6cMji1YsAD+/v547rnnrivcAJyDQ0RERI1MFnBsbW0RGBhocMza2hrOzs5Gx5ujTsuAQ0RERDomX0XVWjhERURERA1M1oPTlH379rX4uRyiIiIiogbS6cFhwCEiIqJ6DDhEREQkOQw4REREJDmSCTi1nGRMRERE9SQTcNiDQ0RERA0YcIiIiEhyGHCIiIhIciQTcGo1nINDREREOpIJOOo69uAQERGRjnQCDoeoiIiIqB4DDhEREUmOZAIO5+AQERFRA8kEHPbgEBERUQPpBBxOMiYiIqJ60gk47MEhIiKiepIJOJyDQ0RERA0kE3DYg0NEREQNJBNw6hhwiIiIqJ5kAo6aQ1RERERUTzIBp5Y9OERERFRPMgGHc3CIiIioAQMOERERSY50Ak4d5+AQERGRjmQCDufgEBERUQPJBBwOUREREVEDBhwiIiKSHAkFHM7BISIiIh3JBJxa7iZORERE9SQTcDhERURERA0YcIiIiEhyJBRwOAeHiIiIdCQTcFgHh4iIiBpIJuCoNVoIAntxiIiISEIBRxAAjZYBh4iIiCQUcADOwyEiIiIdSQUczsMhIiIiQGIBh0vFiYiICJBIwDFXyAAw4BAREZGOJAKOWX3AqeMcHCIiIoJEAo65XPcxOAeHiIiIAIkEHAtz3ceoqtWYuCVERETUHkgi4NhYmAMASqvVJm4JERERtQfSCDgqMwBAWXWdiVtCRERE7YE0Ao4FAw4RERE1kkTAsRV7cDhERURERBIJOByiIiIiIn3SCDgW7MEhIiKiRpIIOLYq3Soq9uAQERERIJGAY2OhAMCAQ0RERDrSCDgq1sEhIiKiRtIIOFwmTkRERHokEXC4TJyIiIj0SSPgsAeHiIiI9Egi4HCIioiIiPRJI+DUTzKuUmug1mhN3BoiIiIyNUkEnIYhKgAoZy8OERHRTU8SAcdcIYeFue6jcJiKiIiIJBFwAMDOgrVwiIiISEcyAcfeUhdwSqoYcIiIiG52DDhEREQkOQw4REREJDkMOERERCQ5kgk4dgw4REREVE8yAcfBigGHiIiIdCQTcDhERURERA0kF3BKGXCIiIhuepILOOzBISIiIskFnOJKBhwiIqKbneQCDntwiIiIyKQBZ926dQgKCoKdnR3s7OwQGhqKv//+u0WvJc7BqVZDqxVas5lERETUwZg04Hh6emLlypWIjY1FTEwMxo4di+nTpyM+Pv66X6uhDo4gAGU13FGciIjoZmbSgDNt2jRMmTIFPXr0QM+ePbF8+XLY2NjgyJEj1/1aFuYKWCkVAICIc3mt3VQiIiLqQNrNHByNRoNNmzahoqICoaGhTV5TU1OD0tJSgx99swd7AwCe23wSpdWci0NERHSzMnnAOXXqFGxsbKBSqbBw4UJs3boVAQEBTV67YsUK2Nvbiz9eXl4G55dN9oerrQqVtRok5ZS3RfOJiIioHTJ5wOnVqxfi4uIQFRWFRx55BPPnz0dCQkKT1y5btgwlJSXiT3p6usF5M4Ucvi7WAIBLRZU3vO1ERETUPpmZugFKpRLdu3cHAAwYMADR0dFYvXo1PvvsM6NrVSoVVCrVVV/Py9EKR1MKkV7IgENERHSzMnkPzuW0Wi1qampa/HxvJysAQBoDDhER0U3LpD04y5Ytw+TJk+Ht7Y2ysjL88MMP2LdvH3bu3Nni1/RysgQAxKQWIa2gEt7OVq3VXCIiIuogTNqDk5ubi/vuuw+9evXCuHHjEB0djZ07d2L8+PEtfs2GHpzk/ApM+egAiipqW6u5RERE1EGYtAfnyy+/bPXX9HJq7LEpr6nD0dRCTOzj3urvQ0RERO1Xu5uD8291slHBxUYpPj6aUmjC1hAREZEpSC7gyOUy/PDQUNw/zBcAEJ3KgENERHSzkVzAAYCebrZ4eFRXAMDpjBLklelWZQmCAA034iQiIpI8SQYcAPBwsEQ/LwdoBWDdvgtQa7SYvPoApqw+gOJKTjwmIiKSMpMX+ruRnhzfE/O/OopvI1NRWFGDs9llAIDFPxwHoJuEvHB0N0wK5CRkIiIiKZFsDw4AjOrhgjv6e6JOK+C3uEzx+MHz+Th4Ph9x6cV4YtNx1NRpTNhKIiIiam2SDjgymQz/mxWEsf6u4jF3OwuDa2rqtDh5qQSCwLk5REREUiHpgAPoQs6DI/zEx988MBgT+7jhjv6eGFcffGZ9Gom7Pz/CkENERCQRkp6D0yC0mzP+M8IPZnIZerrZ4LN5AwEAa/eeR/jZXAC6ejknL5Ug2MsB1WoNFHIZzBWSz39ERESSdFMEHJlMhpdvDTA6HuLtYPB487FL2Ho8AxujLmJoV2d8958hbdRCIiIiak03RcC5kv7ejujv7YDTGaWo1WjxbeRF8dyBpHyczS6Fv7udCVtIRERELXFTj8FYmCuw5dHhSHhjIvp2sTc6v/VYhglaRURERP/WTR1wGpgp5PhodgicrZUY6OOIT+b2BwD8HpcJLSsfExERdTg39RCVPj8Xa0QuGweFXAa1RgtrpQLZpdU4cakYId6Opm4eERERXQf24OhRmsmhkMtgYa7ALfVLyH87zmEqIiKijoYB5womB3YGAHwTeRHrI5JN3BoiIiK6Hgw4VzChj5tYCPDjPUnIKK5CZW2diVtFREREzcGAcwXmCjk+nTcASjM5SqvrMHzlHjy68Zipm0VERETNwIBzFeYKOYL0lo/vS8xDbmm1CVtEREREzcGAcw3BXg4Gj3efyTVNQ4iIiKjZGHCuoVsnG4PH3x+5iJIqNQBAEATsTshBXlmNKZpGREREV8CAcw23h3TB5EB3PDKmG2xVZkjIKsUj38dCEARsPpaBB7+NwYINR03dTCIiItLDQn/XYKlUYN29AwAAtwZ1xsxPDuPwhQL8fTob3x/R7V11OqPUlE0kIiKiy7AH5zr08bDHw6O6AgCWbIpDXHqxeE7DLR2IiIjaDQac67Tolu4YH+CGWo3W4Hg2V1cRERG1Gww418nCXIF19Ztx6hu+cg8rHhMREbUTDDgtYKaQ44HhfkbHl28/Y4LWEBER0eU4ybiFnpvcC45W5vgnIQenMkpM3RwiIiLSwx6cFlKZKfDYuB7o4WpYJ2fihxH480SmiVpFREREAAPOvzZ7iLfB48ScMjz243ETtYaIiIgABpx/bZCvE/Y+PQYe9hYGxwWBy8aJiIhMhQGnFfi5WMOvk7XBsYKKWhO1hoiIiBhwWomNynC+dlJOufh7Tmk1tFcoBMgCgURERK2PAaeVlFbVGTyOzyzBxqiL+OpgCoa8HY41e88bPef3uAz0eXUH/onPbqtmEhER3RRkQgeeLFJaWgp7e3uUlJTAzs7OpG35ISoNL2w9ddVrjr44Dj9EpWHOEG+42lrA9/m/AADmChmSlk9pi2YSERGZXFv8/WYdnFZy10BPONsokV9egxe3nm7ymqkfHUReWQ22HMtAxLO3iMfVmg6bMYmIiNolDlG1EjOFHBP7uOOeQd6wNFc0eU1eWQ0AIK2wEpuOphmc68AdaURERO1OiwJOeno6Ll26JD4+evQolixZgs8//7zVGtZRKeQyfHX/INhbmuOlqb2x6u5++OCuYKPrnt9iOJxVyFVXREREraZFQ1Rz5szBww8/jHnz5iE7Oxvjx49Hnz59sHHjRmRnZ+OVV15p7XZ2KKHdnHHi1Qni49yya+80fiGvAs42qhvZLCIioptGi3pwTp8+jcGDBwMAfv75ZwQGBuLw4cPYuHEjNmzY0JrtkwRXW8MigMlvG08oPnQ+H59HXEB5jeFqrMTsMryz4yyqajU3tI1ERERS0qIeHLVaDZVK19uwe/du3HbbbQAAf39/ZGVltV7rJEoul2HDgkF45fd4dLJVIfZiEVaHJwEAiivVeHaSPwBAqxUwcVUEAMDZWokHR3Y1WZuJiIg6khb14PTp0weffvopDhw4gF27dmHSpEkAgMzMTDg7O7dqA6Vi2WRdaFk4uhsAYEwvV0Q8ews+mzfA4LpP9l3AyHf34HRGCfYm5orHLxVVtV1jiYiIOrgW9eC88847uP322/Hee+9h/vz5CA7WTaL9448/xKErMvTQyK4Y5OeEQA97g+MuNiqsvqcfXv0jHsWVagBAemEVvjiQjGq1VryurNpw6IqIiIiurMWF/jQaDUpLS+Ho6CgeS01NhZWVFVxdXVutgVfTngr9tYb/fheDnfE5AIAerjYor6lDVolugrK/uy2Wju+J8QFukMlkTT5foxWgkDd9joiIqL1oi7/fLRqiqqqqQk1NjRhuLl68iFWrViExMbHNwo0UvTKtD/47SjfPJim3XAw3AHA2uwwPfxeLX2IvNfncZ345gYFv7cLpjJI2aSsREVF71qKAM336dHz77bcAgOLiYgwZMgTvv/8+ZsyYgXXr1rVqA28mXRwssWxKb/i7217xmt/jMoyOFVfW4pfYSyiqVGPO+iOorOVwFhER3dxaFHCOHTuGkSNHAgB+/fVXuLm54eLFi/j222/x0UcftWoDb0bjA9zE30O7Gk7aljcxPLX7TONk5NLqOhxNKTS6Zu3e81j6U9wVdzUnIiKSkhYFnMrKStja6noZ/vnnH8ycORNyuRxDhw7FxYsXW7WBN6NHxnQTfw/TCzsAkJBZihPpxeLjY2lFePk3w72vLhVVoVqtQUmVbtJytVqD93YmYsvxDMRnlt64hhMREbUTLQo43bt3x2+//Yb09HTs3LkTEyboqvbm5uZKYrKvqVkpzbD36TFYOr4nZg/2MjhXUFGL6WsPIeJcHtQaLR75PhZVag1sVWYY66+b/5SaX4G7Pz+CYSvCkV1SjYSsxlBTXceCgUREJH0tWib+yiuvYM6cOXjyyScxduxYhIaGAtD15oSEhLRqA29Wfi7WeHxcDwCApbkCVWrDYHLfV0fF3y3M5djx5Cj8fSoLe87m4ouDKeK5vYm5qK1rXG5eUr8UnYiISMpaFHDuvPNOjBgxAllZWWINHAAYN24cbr/99lZrHOn8+kgo/jqZhQNJ+TjVxCqpBcP90MXBEp6OVkbnopILYKZo7KgrrmLAISIi6WtRwAEAd3d3uLu7i7uKe3p6ssjfDdLHwx59POxx54By7EvMwxvbEsRzQ7s6YcFwXwCAp6Ol0XMjkwtgrWq8zcWVV9+1nLV0iIhIClo0B0er1eKNN96Avb09fHx84OPjAwcHB7z55pvQarXXfgFqka6dbPDACD88OMIPtioz/Ll4BDY9HCpu5uml14PT39sBSoUcOaU1SM6rEI+XXKEHp06jxZz1R9D9xe245/NIXKn+4+6EHINJzkRERO1Ri3pwXnzxRXz55ZdYuXIlhg8fDgA4ePAgXnvtNVRXV2P58uWt2kgy9NKtAXhhSm/IL+tpsbcyF3/v28UeQ7s645N9FwAAtiozlNXUidtBAMBrf8QjOrUQo3t2Qj8vBxy+UAAAOJJciLzyGuNd0PPK8eC3MQCAF6b44+FR3UBERNQetSjgfPPNN/jiiy/EXcQBICgoCF26dMGjjz7KgNMGLg83DWb080BEUj4eGdMd7vYWCOxij9zSatRpBbz11xlxDk5mcRU2HE4FAMRnlsLD/vIwU2EUcM7nlou/f7o/mQGHiIjarRYFnMLCQvj7+xsd9/f3R2GhcZE5ajur7glBnUYrTiye0rczAGBz/RYPDXNwYi4WGTwvs35bCB9nK1wsqERKfgWGXlZkMKO4cUfzwopaVNVqYKlUGFyj1mjxeUQyRvZwQZCnQ+t9MCIiouvQojk4wcHBWLNmjdHxNWvWICgo6F83iv4d/VVTDRzqh68OJOXjk33n8f4/iQAAe8vGYa0pfd3FWjrJeeWoqtVg2scH8dC3MRAEAZeKqgxeM6vE8DEAfBt5Ee/tTMRtaw612uchIiK6Xi3qwXn33XcxdepU7N69W6yBExkZifT0dGzfvr1VG0itw0Fvfs67OxLF35+f7I+3tiVArRHw7ER/HEjKAwCk5Fdg/7k8nMoowamMEuyMz0aGUcCpRtdONgbHDp3Pv4GfgoiIqHlaFHBGjx6Nc+fOYe3atTh79iwAYObMmXj44Yfx1ltviftUUfuh31PTQCGXYZy/K/p7O0KAAF8Xa3EYKjmvAvvPNe5x9fqfCQa7mwO6EPRDVBq6udpgfqgPntt8CnvONj6noqYOZgoZVGaGw1hEREQ3mky40nrgFjhx4gT69+8PjaZttgMoLS2Fvb09SkpKuEXENeSV1WDQ8t0AgKlBnXHvEB+oNVqM6tnJ4Lqc0moMeTscchlwpX05gz3tceJSCYK9HK66ZPzDu4Px3OZT+M8IPzw3STdnS6MV8NepLIzq4YJLRVV4dOMxLJvsj8n1c4UAoLZOiwNJeRje3QUW5gxHRERS0xZ/v1s0B4c6Hv0enEAPe4R2czYKNwDgZmeB0K7OYrixNFcg+sUwg1VWA32dAOCa9XCe/OkEauu0WFe/VB0Afjiahsd/PI67PovE29vPIK2wEo9sPGbwvOc2n8R/vokxeB4REdH1YMC5SSjNGm91WG/Xq1776C2Ny79fnNobnWxVuL++WjIA9HKzve73P5dThsHLd4s7n5/LKUdpdWNNntR8XTFCQRCw9XgGAOCziAviMSIiouvR4q0aqOPZ9tgIlFap0eMaAWVEdxe8NLU3bFRmuGewNwDdflcF5bXo6WYLNzvD+jj6w1nmChnUGuNA8tC3McgtqzE4djqjcZfzW97fh18XDkN5TZ14rKuLDX6JScerf8Rj/X0DMby7y3V9XiIiunld1xycmTNnXvV8cXEx9u/fzzk4EldUUYvBb+8Wg8z7s4Lx7OaTGNHdBRsWDML3UWliT82/4WKjRH65rm7PyB4u+O4/Q/71axIRkem1xd/v6+rBsbe3v+b5++677181iNo/R2sl+nk5IDpVVyxwWrAHQrwd4GKrgkwmw1A/p+t6vbVz+mPRD8eMjjeEG0C3Ymvpz3F4ekIveDgYbypKRESk77oCztdff32j2kEdzGNje+C+r44iyNMeSjO5QT2cHm62WH/fQCzbcgr55TVXeRWdyYHuBo+PvjAOw9/ZYzDUdamoCpeKMpBfXotvHzDctb6sWo2U/IprVk7WagXI5TJkFlchv7yGlZaJiCSMc3CoRUb17ITNjwxD58v2sGowPsANYb1d8eHuJHwUnmR0Xn+ujlwuQ083G5zLKYdMBrjaWaCTjUrcPkJfbGrjViDH0orwS0w68spqsPtMLj6eHYJpwR5Gz6nTaPH4puOISS3Cp/MG4J7PjqBWo8Xep8fAz8W6pV8BERG1Y1xFRS02wMfxqsNFMpkuuDRl44NDYa1U4J07+gIAVt0dggE+jvj5v7rK2AUVtU0+Ty7TbTIqCAJmfnIYPx5Nx+4zuuKCj/14HB/8k4jaOq3Bcz7ecx7bT2Ujt6wGMz85jFqN7vyxy/bjIiIi6WAPDt1QE/u443+zghHsaY/j6cV49teTuDWoMwb7OSH+jUnidQEedtj8yDDxsZlchqYGt7T1c+KPpjS9qetHe87Dy8kKswZ6YcuxSwg/m4sD5/KavLapvbQA3VCWRhBgrpCjqKIWMhngYKW86uf8OTod/yRkY6y/G+YM8b7qtUREdOOZtAdnxYoVGDRoEGxtbeHq6ooZM2YgMTHx2k+kDsNcIcedAzzRw80Wd/b3xJZHh+G9O4Ov+bw1c/rD1VaFHx40XDlVUavB0LfDcffnR6743ISsUhSU1+CFrafw18kslFbrlp7rBygASMmvNHpunUaLaWsOYsx7+5CaX4HJqw9g0qoD2BmfjYNJTe+zdSGvHM9uPondZ3Lx7s6z1/xsRER045k04Ozfvx+LFi3CkSNHsGvXLqjVakyYMAEVFRWmbBbdIHK5DP29HWGpvPb2C7f4u+Loi2EY1kTtm+xS47k5+lLzK/DFwRRUqxuHqnycrTDAxxFr5oTAy0k3rJZaYPzf2d+nsxGfWYqM4iqM+d8+ZJdWI7u0Gv/9Lhb3fhmFS0WVEAQBy/9KwPqIZEReKMDfp7LE5xdXqlGt1pVJqKipw9wvjuCrgykAgGq1Bq//GY/D3JCUiOiGM+kQ1Y4dOwweb9iwAa6uroiNjcWoUaNM1CrqCAb4OOLVaQG4bc0hAECQpz0Wju6GRzceQ3RqkdEQVm93XZ2FW4M84OtsjVs/PoiLBRXYm5iLS4WVuHeoDwQB+Dwi+arv+92Ri5gc2BnrD6Rc8ZrzueXo6WaLLcczcOh8AQ6dL8CC4b746lAKvj6Uiq8PpSJ15dRrfsaLBRXo4mAJMwWnyhERXa929S9nSUkJAMDJqek6KjU1NSgtLTX4Iel7YYpuo86G/bTmDPHG5keGIcjTAb0764LLXQO9MKybMwCgvKYOFbUa+Dhbia/h6dg4GbrheH55LRZ8HY2Xf49HRFI+NkWn41RGCayUCiy/PbDJtvwcnY6EzKv/d3frxwfxwIZo1KgbC17mldUgPsP4eWkFlVBrtEbH/zyRidHv7cNHe85f9b2IiKhp7WaSsVarxZIlSzB8+HAEBjb9x2XFihV4/fXX27hlZGoPjeyKmf09YSaX4fCFAkwIcBPPffvAYMReLMLEPm6Q1a+wavDsRH+xgGCfLo2VMm0tzNHJVoU8va0j/onPxs74bADAUxN6Ye4QH/RwtUVUcgHe33VOvK6oUo09Z3Ou2eaD5/MR5NlYGDM+s1RcvQUAao0WUcmFuPfLKNwe0gUf3t3P4PnP/HoCAPBReBKWju95zfcTBMHo8xMR3czaTQ/OokWLcPr0aWzatOmK1yxbtgwlJSXiT3p6ehu2kExFJpPBxUYFByslpvTtbDBk08lWhUmB7uIf96lBnQEA/x3VFVP6uuO3RcPx8q0BmNGvi8FrNvT2NNgYlYb88loo5DLMG+oDABjs54QHR3aFk7USdhZmcLVVAYC4LP1yZnLDgHEup0z8PSGrFOXVjfts5ZbViPWBth7PwLAV4fjiQDKKK2vx/OaTBvOHrrWbypZjlxD0+j+IaGK12L7EXPz3uxgUNKPgIhGRlLSLHpzFixdj27ZtiIiIgKen5xWvU6lUUKlUbdgy6mjevSMIj43tDv/6OTf9vBzQz8vB6Lpxvd3we1ym0XEfZyuDndctlQr89fgIAMCzv5402jBUX39vRxw1KERYLP4en1mCS8WNq7ayS6qg0QsumSXVeOuvM9iVkIOoy+YP7T+Xh1W7k7BguC/OZJVh/jAfdLa3hCAIyCiuwpvbElBWXYf7vjqKlBVTkF9ei8MX8jGlb2fc/3U0AKBOI+DL+wddse1ERFJj0oAjCAIee+wxbN26Ffv27YOfn58pm0MSYK0yE8PN1Yzu0anJ4z1cjQsTdrbXzd/xdmqc0yOT6eYEFVeqxWMhPg4GAadQr1jhvsQ8VNY2zsnJLqlBXRNzby4PNwDEkPLEpjgAQE5pNT68ux8+3Z+Md3YYLkv/40QmXtx6GuU1dajRK3i4N7HpXqcr0WoF1Gq0sDC/9oo3IqL2yKRDVIsWLcL333+PH374Aba2tsjOzkZ2djaqqpouwEbUWuytzPHMxF64LdgDj43tLh7v3kTAaaA/aXlCgBv2PDUGvy8aLh7zc2562welmdwg3AC6IoM5pS0bNopLLwYAo3AD6EJQeY1uKCw2tbFSs1bQ7dnVXA98E42hK8JRUnn157zxZwJe/zO+2a9LRNRWTBpw1q1bh5KSEowZMwadO3cWf3766SdTNotuEotu6Y6PZodgsN7u51cLOPo9OEO7OsPJWolgLwdsengodi8dbTC0pW/5DONJ8x+FJ12zns+VmCuaN5n4SEqBweOx7+9HTjPeUxAE7EvMQ3GlGuGXTaj+5nAqPqifdF1QXiMufc+7ytAdEZEpmHyIisjU9Ofo6IeYy3k6Ggacy3+vVmuMnuNio8SdAzwhCMD201koqqjFiUslYnXlK3liXA8M9HXEvC+PisfuHeqN74+k4VJRFapqjd/rchcLDCs155XV4MuDKXhhSu8rPudcThmOpzW9R5dao8Wrf+h6a24P6YKKmsbPkFlchU62nB9HRO1Hu1lFRWQqthbmWDq+J+4a6IkQL8crXtfd1Qaejpbo3dkOvdxsjc4HdrHH+vsG4r+ju4rH+nk5QiaT4a5BXtiwYDAW3dLd6HkNGur8AMB/R3fFSL15Qgq5DC9NDQAAVNZqEHsdG4UO9nPCmjkhAHQrrhrq7qg1WoMhqGq1BhM+jMBzm0+Jx4r0zuvPKSqvrkO23m7vGcUcViai9oUBhwjA4+N64N07gyGXX3n4x8JcgT1PjcFvi4Zd8brxAW4Y28tVfLysvkhhg1v8XfH8ZH+8e2cQgj3tDYa1zPWWv1spdZ2rX90/EEozOd65IwgW5gq42el6SfZdZdKwtVIBP5fG+UBdHCwxsY87XGyUyC+vFffUeuaXExiyYre4nH273pYTDXL1hrT0h6GKq2oNNiu9VGS8rxcRkSkx4BBdB6WZHCqzq68sGuTrhEfGdMO6uf3RrZPhnB5zhRwLR3fDXQO98PviETiybBym9/PAxgeHQNXEHJ6x/m5IfHMS7hygK5/QMEy256xhwLHS29/LTCFHYJfGIoNudhYwV8gxsY87AN2yc61WwK6EHFSrtdgcewnVag0+3X/B6P1PZZTgfG45AMOAU1hRi0z9HpyiK/fgVKs1eO2PePwae+mK1wBATZ0G7/+TKG6z0VSFZyKi5mLAIWplcrkMz03yx+S+na95rZO1EqvvCcHw7i4YX1+huaGXpoF+heKGLSeS8w03Cu3v3Ti0ppDLDKofd+uk681pGPLacDgVs9cfQUX9PJ6/T2djxfYzOJdTbtS+wxcKEPbBfqQVVBr24FSqkVWs34PTdMARBAELv4/FhsOpePqXE1edd7fxSBo+3nMed30WiW0nMxH46k78b2fiFa8nIroaBhyiduKZib3w7KRe+Onh0Cte08fDsMbPrwtDcddAT7xzZ5B4zNVWBT8Xa/zz5Cg8M7EXpgV7AABC9ao369fbSSusxPdRaQCA+0J9mnzfv05lIU+vGvL3Ry7iN71CiQ1zcPaczcHm2EvikvTYi0XYl9hYYflYWjGK9ObyNCiqqBVXZwHA4h+Oo6ZOizV728deXD9Hp2PJpuOorWOvElFHIRM68FKm0tJS2Nvbo6SkBHZ21y7uRtTRVas1GLR8N8qq6zA1qDPWzukvnvs9LgPv7kjEp/cOQF+9fbD03fN5JI4kGxcTBHS9SVseGYYx/9vXorbdFuyBP07oQk8XB0uozORGPU0N5+wtzRHgYYe3ZgQis7gKY9/ff8XXTVkxpdn7bJ3OKMHb28/glWkBSMmrwOt/JuDNGYFi71hL+T7/FwDgf7OCxeFCImq5tvj73S62aiCi5rEwV+D3RcPx9aFULBzTzeDc9H5dMP2yPbcu9+Hd/XAwKR/P/HoSANDLzRaJ9ZOMQ7s6w+sqy+SvpSHcAMarqszkMtRpBfFcRnEVErJKUVunxYjuLuJ1L03tjW0ns8RihgCQV16DGrUWz/56Eg+M8EM/Lwd8vCcJ04I9MMjXyeB9bltzEFoBWFJf9Tm7tBoPfRuDQ8+PRRcHS7SE/v8PWFLV/GKJRGRaHKIi6mC6drLBmzMCW/QHu7O9JWYN9MLjY7ujq4s11s7tLxYOHNbdGQq5DFODOje7pk3vznYIf2o07ujf2KtxSy/DbTD83W0xa6BXk8/fcTpbXMV171BvPDiyK965I0jc2BTQ1fN5d2ciIpML8NC3MfjiQDK+jbyIWZ9G4peYxg13C8prUJ+hkF5YifzLhtRaqkKv5lBziixGXii46io3Imob7MEhugktndALSyf0AgA8MMIP+87mYVL9Kqs1s0Og0Qp45teTOHGpGH8sHoG/TmYa1McBgBn9PPDh3f0gk8nwv1lB6ONhB1sLMwzydcLB8/tha2GOPxYPh5O1EhuPpDXZjlqNFr/FZQAAfJx0k6F7udvi6IthmPdlFA4k5SMlvwIxent8fRaRLP7+3OaT8HCwxLGLRXhfbw5PpVpjEEwiLzRWdU7Jr8BfJzPx4MiuzdprS38n9mvNwamt02L2+iMAgCPLxsHd3qLJ62IvFsLWwhw9m6inREStgwGH6Ca3bHJvLJvcWN1YJpPBTCHDh3f3gyAIkMlkGN3T1eh53TrZiHNjZDIZHhjRuFnub4uGw1ppJi5rd7JWiueUCjkmBbpDJgN+j8tEfrlu0rG3s+HwmI+zFQ4kAXvO5CKrxHiLiSF+TohKKcTSn+OM9vVqGFWyNFegSq1BXHoxRryzB1/MH4gHvo5GZkk1CivUeGVaQJPfSfiZHBxIysfzk/3F9gHXHqLSrw108lIx3O3dja45l1OGO9ZFAgBSV0696usRUctxiIqIrqghwLjbW+Dxsd0xskfjfBm/Tk1vLgoAfTzs4atXbHBkDxdYKxUI8XbAueWT8dHsEITqbXcBGG5mCgC+9ZuX7ojPBgAM8DGsMr1guC5QNYSbLg6WmNLXMFCEBbjBwlz3z9yloip8G3lRrN/T0HMEAGv3nsewFeFIzitHeU0d/vNNDDYcTsVP0ekGPTjXCjj6y+VPXCpu8prdZxr396qsvfqWHYBuDtD2U1lIL2QxRaLrwR4cImqWpRN6oaxajb6v/QMA6HyF4ZemuNpZYP+ztxgMCY31N+wVunwfsLH+rvjfP4moVmthaa7AWzMCkZRbjiWbjmPp+J5GgejJ8T1x5wBPjHlvL1Lr9+Ea4ueETjYqfHUoBQAMig0WVtRi0qoICALEidaXr+b6JjIV3fWKNV4r4OiHkLV7L0BlpsD8UF/YWzVuw3HsYrH4e25pDXxdDP8ZFgQBW45loJe7LQK72OOPE5l4YlMcXGyUiHlpPCpr67BqdxImB7ojxPvKW4sQ3ewYcIio2WxUZvByskReWQ16d76+pZ0uNoYTl13tLNDD1QZJ9ZWSG7anaNC1kw3+XDwCn0Uk4/aQLujd2Q69O9thTK9OsFWZoeqyzU0bdoIf1bMTUiMvwlqpwN2DvKDWaOFkbY7//XPOaA7N2eyyq7Y5Oa8CyXmNS92bCjjVag1e/zMBxZW1RnNuPth1Dr/GXsKKmX3hYKWbcxN5IV88n1NabdDTBegmXj/1ywkAuiGs3+vrDTUMlX267wI+j0jG5xHJHOIiugoGHCJqNplMhh1PjIJGEIwCSUt8NDsEd647jEmBTVd97uFmi//NCjY4Zmeh6w2xUprBwlyOarUutDRUbH5kTDfYW5rj3qE+MFfIYa6QY95QX/zvn8ZJyOP8XZFVUg0nayUOns/H5V6dFoDvIi8a1fEpqVKjWq1BtVoDByslzuWUYdmWU1fd/DStsBJzv4iCtVKBHx4aajD5Obesxuh6/cKItXVag9VggiAgPrP0iu9FRI0YcIjoulirWu+fjd6d7RD9Uhgsm7GaqSl1msYaNbb1waezvSWeql8h1kB/iAgA1szpD0ulAhqtgG4vbBeP26jMsPHBIQj2csDw7i546ucTOJVRIp4vqVRjzvojiEsvxqwBXgg/m2sQQBq8d2cQpvTtjL9PZ+Pp+t6YiloNNkWnG1z3wtZT0GgFzAhprF+UpjfMNeHD/eJwW8Nr6G/KWlunNdiwlYga8X8ZRGRSVkqzZlcqvlxD8cDmcKwPOb0728GyfnNShVyGr+8fhOn9PHDi1Qk48eoEBHs5AAB6utninTuCDF4jOb8Cx9KKoRWAn2LSkV9eA6VCjohnbjHY5qKHmy2sVWaY0c8DtwY19k79eNRwuXxZdR2W/BSHS0W6uj2v/xmPyOTGJe364QYA1uw5j6TcxmG1zPqCikk5Zai+bMiugVqjxemMEmjqv6vskmp8HJ5kMHn6RqpWa1Bec+3J1EStjVs1EFGHtWr3OazanYS7Bnri3TuDr3rt0ZRCfHkwGa9M69PsIomX9/A0peG9c0qrMeTtcMhlwPGXJxj0Gp1IL8b0tYea9Z7Xw0wug6utCpkl1bgv1AeTAt3x49F0TA50x+RAd6QXVmHBhqO4kFeBZyb2wqJbumPy6gM4k1WK6f08sPqeEIPXSy+sxM74bMwa6AV7S3NotQL2J+Xhz7hMOFkr8U9CDn7+b2iT9X02HEpBzMUifHBXP7FXSRAETF97CNkl1di1dDTsLc2Nnkc3J27VQER0FY+O6Y5Bvk4Gu6lfyWA/Jwz2c7rmdfoUchlenNIbf50y3D7iq/sH4oENMQCA8QG6peludhbY/vhI1NRpjIbEArvYQ6mQo1ajmy/kaqtqcv4NAAR0tkNlbZ1R701T6rSCuOz928iL+DZSV7H5zxOZeGp8TyTnV+BC/STpoymFWHQLcCZLN4fn97hMjA9ww8Q+7uKw1xvbErArIQerdyehi6MlUgsqxDlODX6NTcfisT3w5rYE7Dmbi5//GwoXGyVe+zMBADChjztuq9/gNbesBicv6Yb49p/LE48TtQUOURFRh6U0k2N4dxdxyOlGeGhUV2x9dJj42MlaibH+blg22R+zB3thjN7WFAEedk0u3VbIZfDTWy3Vt4vxZqheTpbY+ugwbH9iJO4d2jjcdfr1iZjY5/o3C31/1zlsPd5Y6+dsdqm4y3uDxT8cx4rtZ1FRU4dD5/PFGj1lNXU4m11mFG4AoFqtxaWiSnx5MAUp+RX4PS5DDFkAkFP/+5o9SZi0KkI8HpdWjA93ncOU1QfwwtZTBnt8VdVqsPC7WLxRH5KuJbe0GlW1TQ/JETVgwCEiugb9OUINxQ7/O7obVswMMpj0ezUNy9gBXRC63Kq7+4nhaHq/LnCzU+Heod6wUZkZLbHXd+aNSeIKMgDYsWSkQSBqeG5OaY3BCq0GXx1Kwet/xmPuF1FiBeiXpvZGv/q5SJe7VFSJjVGNc4lOZ5QgMbtxZVdKQQWq1Rr8759zKKpsDFT/JGRjdXgSErJK8UNUmkH16Te2JWBHfDa+OpSCkko1Fm08hnd2nG3y/ROzyzDy3b147MfjV/xOWoMgCMguqUYHnsVx02PAISJqhmnBHnC2VuK5Sf4ter5+CBnVsxOWju+J+XoTk4M8HcTfO9mqEPVCGN6a0RcA4GBlPHfFSqnAk2E9YalUGOwC36t+cnRDVeenJjQWRbxSKPg5prEAYoi3Ax4c2RU/PDSkyWvTi6qw83S2+DgqpdCgntD5nHIkZBkvZdev8gzoVotVqzUorKjFpujGwPRTTBr+OpWFdfsu4OtDKXj8x+PYXF+g8fCFfDz1Sxxq6rTYfSYHH+46hyi9SdmXO51R0qxq0U15YetpDF0Rju2nsps8r9UKyC3T9VbtPZuLp385wcnU7Qzn4BARNcPHs0P+1bLsbno9OO52Fnh8XA8Aup4db2frq/YEVdQYD8fEvz5R/P2lqb2RWVyFxWN7QCaTwcFKiU/mDkCdRgszhRzhZ3JxsRlzegBgaP0WGlZKM4z1d8Wes7n46v6BcLZWYfraQziXXYZyvdCQVVKNPWcad08/l1uGk3rzla7krs8i0a2TNd6YHgj9TpLdCY2v9Xr9kNU/Cdno6WaLOeujDF5jdXgSVocnYffS0fBysoTKrHGo8s8TmXjsx+OYNcAT780ynoAelVyA5zafxKyBXnhkdDfI5TKD5zaseNt+KgtTg4zrNC3bcgo/xaTjl4WhWLAhGgDg52KNRbd0v+Znbw5BELDoh2OwMFfg/VnBLV5peDNjDw4RUTP9m5oz7naNK4862TYOOc0L9cXonp2aeoqoj96Q1qwBnvjuP4Mhk8nEP3rdXW3xz5OjjSbxmtWHptE9Xer/byccen5sk+8xNagzFgz3xSNjuonHVt3TD1seHYax/m5iL1FZTR0EQTdRemD9/mAxeoUOiyvV+PNklvjYxUaJWQM8m3zPC3m6OTz6jurtHN+gWq3F14dTmnwNAAj7YD/uWHcYZdVq7IzPRl5ZDZ76WVd/6JfYS6jTGM8l2nA4FakFlXhvZyKe/uWEuIweAP7Sa3/aZXuACYKAyto6/BSjq2n03o5E8VxOqfGmsC11saAS209lY8uxjCtOSKerYw8OEVEbCPF2RGAXO7jZWhjsydUct4d0QXlNHUK7OcPf/fqX1N471Aeje7rCy8nSqCdgaFcnDPZzxmNjuxv1ItlZmIsr1BytzGGtVIiVmAM87DC8m4sYbpQKOTo7WOBiQaVY2XnNnBDc0ssVx9KK8Ev9MFNPNxucyykX30N/eKwpSjM5auu02HYi66rXnc4oFfdJu9yJS8UY4KNbQXfqUgke33QcKXpVqrcczwBkwHt3BkMhlxnUGkrMKUNNnUbsHXr9zwR8E5kqnk/Ob/ws6suC1LItpxCVXIAtjw7DHycycSK9BMtvD2zW/b+oF6wSskrhZtf8vd9Ihz04RERtQGkmx5+LR+DL+wdd93PNFHIsGO7XonAD6CZJeztbGYUbRytzbHo4FEvH97zmZGmZTGYw1yegsx0mBTbu3j6utys2PjgEvdxsAQDO1kqM7N4J1iozDPZzgkwGyGTAhAB3o9cGcMVerFduDQAAcYn9ssn+eFSvl8nCXI7ene3EXeObsl9vcvVXh1IMws2aOSFQyGXYciwD6w8ko6ZOY7BEv7ZOi14v7UDkhQIcSyvChsOpBkNqDXuEAcCPR9Px2h/xKKqoxe6EHPx4NA3J+RU4kJSPV36Px+Zjl7DpsmKPlzt0Ph8R5/JwsaCxjWeamNNE18YeHCKiNtJe5lGsv28g3tgWj/dn9buu5z0w3A/Pbj4JQNeD4+VkhZE9XBCdWoglYT3h6WiFv58YiazSajhbK8WeCpWZAhHP3IKaOi3OZjf9xzqstyv2n9MFEUcrc9wz2Bs+TlaY2McdL/9+WgwVfi7WyNEbsjnzxiTIZDIUVtQiPrMEKjMFPtiViD4e9vBytMRrfyZg87EMPDauB8wVcoOwcM8gL9wa5IGiilq8/Hs8Pg5PQlm1GhqtAFuVGSyVCnF4aMuxS8huxhDUhsOp+OtUllg5G4D4uQBgb2Ie7h/uh9JqNdR1WjjbqBB7sQiVtXXwdbbG3C9084wm9WkMgifTSzD3iyNwtFLi49kh4n9H4Wdy0NnesslVecRKxkREdB1+O56ByAsFeH16H1iYK8StGK62lF1fTGoh7vw00uh49ItheG/nWZRV12HRLd0RqFcraPraQzhRP3F515Oj4ONsjVf/iMfIHi6Y0rfpjVoB3TYRw1fuQUFFLRbd0g1ns8oQflY3ifmZib0wZ7A3HK2VqFZrMOit3SjTWwXV31u3H9nHe84D0C23L6ioQUv+YqrM5KjR28l+gI8jskuqUVatxqfzBmDO+ijIZMCMfl0Mahc15bv/DMbIHp1w8lIxblujq46tv6t87MVCWKvM8PzmUwjr7YrFY3tcV1sbNnTt5W7b7BIILcFKxkRE1K7MCOlisDmohbniuuYU6W/zsP6+gfj6UApcbFRwsVFecbuN0T1ccCK9GDIZ4OVkBaWZHCtm9r3me1mYK3D/MF+8v+sc1u69IB43k8sMVjtZmCswe4g3Po9IFo85Wavw+Lge6O/tiAUbosVNVX2drZpVZVqffrgBYLD7fMPKMEHANcMNAHwekYyRPTrh4Pl88Vi1WgMLc4W4cqxBXHoxpvfrYjC0eC2fRSRj5d9n8d9RXbFsSu9mP6894hwcIiJqM/qTZUd0d8EPDw3FR3rDLk0Z11tXuLCnq+11T9B+aFRXsThjg9v6GW8Z8ezEXtj44BA8PaEnlGZyzBniBXOF3KBSNQAsvWynekAXmJrj5VsD8NT4nte8TqnXczLI17Ay9sHz+SitViNVbx5Rcl4FSqrU4soxffqhDQAqaurw2h/xYsgSBAF7zuagpFKNwoparPxbV2Dxs4jkK27g2lFwiIqIiNpURnEVBEGAp2PzexYizuWhi6MlunWyufbFl6mp02Dv2TwEdrHD3sQ8TAxwg+tVViU11A9qMGX1ASRklcLZWonoF8PQ9bINWF1sVOjlboND5wswe7C3WENnZA8XHEhq7Gn5ZWEo+ns7YuYnh3Cifo+uyw3ydcSTYT3xwtZTcLBS4vP7BuD2tYfhYGWO4ko1Moqr4GytREFFbZPPv5yLjQrRL44TA+TH4Ul4f9c5AMDZNyfhy4MpeG9nIiYEuKGftwPe1Vv2vvqefpjer0uTr/tvtcXfbwYcIiKiq4i9WITvIlPx9MRe8HS0wubYS/j6cApOZ+gmLPdwtcEvC0NxqagKlkoFxr2/HwAwP9QHZ7LLcDSlEPOG+uCN6X0gk8lQU6dB7MUicXjKy8kS6YW6Ss/3D/PFa7f1gSAIEARALpehtk4LM7kMT/4ch9/jMq/YTqWZHB72FkZDaLuX6uYtfXkwBev2XUBJlW4LjdemBeDt7WfFFWqhXZ0RmVwgBqjurjb4+4mRN2QuTlv8/eYQFRER0VUM8HHEqntCxB6nOwZ4YttjI8XzjtZKOFgpEdjFHt56810szBX4+v5B2P/MGLw5I1DsRVGZKcT6QgAwZ3Djlh0NRR1lMplYXVlpJodcLjN4TlO+e2AwJuot3R/aVVf7J+yDCAxbuQcr/z4rhhtAt6y9Vq92T8Ow1RfzB8LJWonzueXiDvUdEQMOERFRCzw00g9mcplYqweAQW9HgIcdrFVm8HG2NnquhbkCH94djOcn++PuQV7i8avVOtIPOH8sHo7FehOl358VjCFdnfHAcD+42akwP9QHoV0b5x7lXVYN2UwuQ2JOmcGxWo0Wne0t0M/LAc9M1M01Op9bjo6Kq6iIiIha4IUpvbEkrCesVYZ/Sjc/MgwxqYWYFmQ8mVnf7SGNW1jMGuCJkir1VWvaBHaxwzMTe8HNzgJBng64kNcYPoK9dMvq3ewsEPVCGADdzusf70lCndZwJsrzk/1xJLmgyd3lh3d3gUwmw10DvdDL3faavUbtGQMOERFRC8hkMqNwA+iGtAb4XF8waGpD0KbeT395u7N1Y+2hri7Gk697udsi9uXxWLblpLgrevLbUyCXy9DFwfIKAUe32aqiGUNi7R0DDhERUQc0orsL7h/miyBPe4Pd0PXZW5pj2eTeOJdTjrsGeorX3RrUGV0cLeFhb4m3/krAtvoNRod3c2nydToiBhwiIqIOSC6X4bXb+lzzOi8nK+xeOtrgmEzW2EPTUAiwp5vNVZfPdzScZExERHQTG+vvCpkMuGeQt6mb0qrYg0NERHQTG+TrhPPLp6CZBZk7DAYcIiKim5xCaukGHKIiIiIiCWLAISIiIslhwCEiIiLJYcAhIiIiyWHAISIiIslhwCEiIiLJYcAhIiIiyWHAISIiIslhwCEiIiLJYcAhIiIiyWHAISIiIslhwCEiIiLJYcAhIiIiyWHAISIiIslhwCEiIiLJYcAhIiIiyWHAISIiIslhwCEiIiLJYcAhIiIiyWHAISIiIslhwCEiIiLJYcAhIiIiyWHAISIiIslhwCEiIiLJYcAhIiIiyWHAISIiIslhwCEiIiLJYcAhIiIiyWHAISIiIskxacCJiIjAtGnT4OHhAZlMht9++82UzSEiIiKJMGnAqaioQHBwMNauXWvKZhAREZHEmJnyzSdPnozJkyebsglEREQkQSYNONerpqYGNTU14uPS0lITtoaIiIjaqw41yXjFihWwt7cXf7y8vEzdJCIiImqHOlTAWbZsGUpKSsSf9PR0UzeJiIiI2qEONUSlUqmgUqlM3QwiIiJq5zpUDw4RERFRc5i0B6e8vBznz58XH6ekpCAuLg5OTk7w9vY2YcuIiIioIzNpwImJicEtt9wiPl66dCkAYP78+diwYYOJWkVEREQdnUkDzpgxYyAIgimbQERERBLEOThEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ57SLgrF27Fr6+vrCwsMCQIUNw9OhRUzeJiIiIOjCTB5yffvoJS5cuxauvvopjx44hODgYEydORG5urqmbRkRERB2UyQPOBx98gIceeggLFixAQEAAPv30U1hZWeGrr74yddOIiIiogzIz5ZvX1tYiNjYWy5YtE4/J5XKEhYUhMjLS6PqamhrU1NSIj0tKSgAApaWlN76xRERE1Coa/m4LgnDD3sOkASc/Px8ajQZubm4Gx93c3HD27Fmj61esWIHXX3/d6LiXl9cNayMRERHdGGVlZbC3t78hr23SgHO9li1bhqVLl4qPi4uL4ePjg7S0tBv2BVHzlJaWwsvLC+np6bCzszN1c25avA/tB+9F+8F70T7o3wdbW1uUlZXBw8Pjhr2fSQOOi4sLFAoFcnJyDI7n5OTA3d3d6HqVSgWVSmV03N7env/RthN2dna8F+0A70P7wXvRfvBetA8N9+FGd0yYdJKxUqnEgAEDEB4eLh7TarUIDw9HaGioCVtGREREHZnJh6iWLl2K+fPnY+DAgRg8eDBWrVqFiooKLFiwwNRNIyIiog7K5AHn7rvvRl5eHl555RVkZ2ejX79+2LFjh9HE46aoVCq8+uqrTQ5bUdvivWgfeB/aD96L9oP3on1o6/sgE27kGi0iIiIiEzB5oT8iIiKi1saAQ0RERJLDgENERESSw4BDREREktOhA87atWvh6+sLCwsLDBkyBEePHjV1kyRlxYoVGDRoEGxtbeHq6ooZM2YgMTHR4Jrq6mosWrQIzs7OsLGxwR133GFUuDEtLQ1Tp06FlZUVXF1d8cwzz6Curq4tP4qkrFy5EjKZDEuWLBGP8T60nYyMDNx7771wdnaGpaUl+vbti5iYGPG8IAh45ZVX0LlzZ1haWiIsLAxJSUkGr1FYWIi5c+fCzs4ODg4O+M9//oPy8vK2/igdlkajwcsvvww/Pz9YWlqiW7duePPNNw32NeJ9uDEiIiIwbdo0eHh4QCaT4bfffjM431rf+8mTJzFy5EhYWFjAy8sL77777vU3VuigNm3aJCiVSuGrr74S4uPjhYceekhwcHAQcnJyTN00yZg4caLw9ddfC6dPnxbi4uKEKVOmCN7e3kJ5ebl4zcKFCwUvLy8hPDxciImJEYYOHSoMGzZMPF9XVycEBgYKYWFhwvHjx4Xt27cLLi4uwrJly0zxkTq8o0ePCr6+vkJQUJDwxBNPiMd5H9pGYWGh4OPjI9x///1CVFSUkJycLOzcuVM4f/68eM3KlSsFe3t74bfffhNOnDgh3HbbbYKfn59QVVUlXjNp0iQhODhYOHLkiHDgwAGhe/fuwuzZs03xkTqk5cuXC87OzsK2bduElJQU4ZdffhFsbGyE1atXi9fwPtwY27dvF1588UVhy5YtAgBh69atBudb43svKSkR3NzchLlz5wqnT58WfvzxR8HS0lL47LPPrqutHTbgDB48WFi0aJH4WKPRCB4eHsKKFStM2Cppy83NFQAI+/fvFwRBEIqLiwVzc3Phl19+Ea85c+aMAECIjIwUBEH3Pwa5XC5kZ2eL16xbt06ws7MTampq2vYDdHBlZWVCjx49hF27dgmjR48WAw7vQ9t57rnnhBEjRlzxvFarFdzd3YX33ntPPFZcXCyoVCrhxx9/FARBEBISEgQAQnR0tHjN33//LchkMiEjI+PGNV5Cpk6dKjzwwAMGx2bOnCnMnTtXEATeh7ZyecBpre/9k08+ERwdHQ3+bXruueeEXr16XVf7OuQQVW1tLWJjYxEWFiYek8vlCAsLQ2RkpAlbJm0lJSUAACcnJwBAbGws1Gq1wX3w9/eHt7e3eB8iIyPRt29fg8KNEydORGlpKeLj49uw9R3fokWLMHXqVIPvG+B9aEt//PEHBg4ciFmzZsHV1RUhISFYv369eD4lJQXZ2dkG98Le3h5DhgwxuBcODg4YOHCgeE1YWBjkcjmioqLa7sN0YMOGDUN4eDjOnTsHADhx4gQOHjyIyZMnA+B9MJXW+t4jIyMxatQoKJVK8ZqJEyciMTERRUVFzW6PySsZt0R+fj40Go1RtWM3NzecPXvWRK2SNq1WiyVLlmD48OEIDAwEAGRnZ0OpVMLBwcHgWjc3N2RnZ4vXNHWfGs5R82zatAnHjh1DdHS00Tneh7aTnJyMdevWYenSpXjhhRcQHR2Nxx9/HEqlEvPnzxe/y6a+a/174erqanDezMwMTk5OvBfN9Pzzz6O0tBT+/v5QKBTQaDRYvnw55s6dCwC8DybSWt97dnY2/Pz8jF6j4Zyjo2Oz2tMhAw61vUWLFuH06dM4ePCgqZty00lPT8cTTzyBXbt2wcLCwtTNualptVoMHDgQb7/9NgAgJCQEp0+fxqeffor58+ebuHU3j59//hkbN27EDz/8gD59+iAuLg5LliyBh4cH7wOJOuQQlYuLCxQKhdEqkZycHLi7u5uoVdK1ePFibNu2DXv37oWnp6d43N3dHbW1tSguLja4Xv8+uLu7N3mfGs7RtcXGxiI3Nxf9+/eHmZkZzMzMsH//fnz00UcwMzODm5sb70Mb6dy5MwICAgyO9e7dG2lpaQAav8ur/dvk7u6O3Nxcg/N1dXUoLCzkvWimZ555Bs8//zzuuece9O3bF/PmzcOTTz6JFStWAOB9MJXW+t5b69+rDhlwlEolBgwYgPDwcPGYVqtFeHg4QkNDTdgyaREEAYsXL8bWrVuxZ88eoy7DAQMGwNzc3OA+JCYmIi0tTbwPoaGhOHXqlMF/0Lt27YKdnZ3RHwpq2rhx43Dq1CnExcWJPwMHDsTcuXPF33kf2sbw4cONSiWcO3cOPj4+AAA/Pz+4u7sb3IvS0lJERUUZ3Ivi4mLExsaK1+zZswdarRZDhgxpg0/R8VVWVkIuN/zzpVAooNVqAfA+mEprfe+hoaGIiIiAWq0Wr9m1axd69erV7OEpAB17mbhKpRI2bNggJCQkCA8//LDg4OBgsEqE/p1HHnlEsLe3F/bt2ydkZWWJP5WVleI1CxcuFLy9vYU9e/YIMTExQmhoqBAaGiqeb1iePGHCBCEuLk7YsWOH0KlTJy5P/pf0V1EJAu9DWzl69KhgZmYmLF++XEhKShI2btwoWFlZCd9//714zcqVKwUHBwfh999/F06ePClMnz69yWWyISEhQlRUlHDw4EGhR48eXJ58HebPny906dJFXCa+ZcsWwcXFRXj22WfFa3gfboyysjLh+PHjwvHjxwUAwgcffCAcP35cuHjxoiAIrfO9FxcXC25ubsK8efOE06dPC5s2bRKsrKxunmXigiAIH3/8seDt7S0olUph8ODBwpEjR0zdJEkB0OTP119/LV5TVVUlPProo4Kjo6NgZWUl3H777UJWVpbB66SmpgqTJ08WLC0tBRcXF+Gpp54S1Gp1G38aabk84PA+tJ0///xTCAwMFFQqleDv7y98/vnnBue1Wq3w8ssvC25uboJKpRLGjRsnJCYmGlxTUFAgzJ49W7CxsRHs7OyEBQsWCGVlZW35MTq00tJS4YknnhC8vb0FCwsLoWvXrsKLL75osKyY9+HG2Lt3b5N/F+bPny8IQut97ydOnBBGjBghqFQqoUuXLsLKlSuvu60yQdAr/UhEREQkAR1yDg4RERHR1TDgEBERkeQw4BAREZHkMOAQERGR5DDgEBERkeQw4BAREZHkMOAQERGR5DDgEBERkeQw4BBRh+Lr64tVq1aZuhlE1M4x4BDRFd1///2YMWMGAGDMmDFYsmRJm733hg0b4ODgYHQ8OjoaDz/8cJu1g4g6JjNTN4CIbi61tbVQKpUtfn6nTp1asTVEJFXswSGia7r//vuxf/9+rF69GjKZDDKZDKmpqQCA06dPY/LkybCxsYGbmxvmzZuH/Px88bljxozB4sWLsWTJEri4uGDixIkAgA8++AB9+/aFtbU1vLy88Oijj6K8vBwAsG/fPixYsAAlJSXi+7322msAjIeo0tLSMH36dNjY2MDOzg533XUXcnJyxPOvvfYa+vXrh++++w6+vr6wt7fHPffcg7KyMvGaX3/9FX379oWlpSWcnZ0RFhaGioqKG/RtElFbYMAhomtavXo1QkND8dBDDyErKwtZWVnw8vJCcXExxo4di5CQEMTExGDHjh3IycnBXXfdZfD8b775BkqlEocOHcKnn34KAJDL5fjoo48QHx+Pb775Bnv27MGzzz4LABg2bBhWrVoFOzs78f2efvppo3ZptVpMnz4dhYWF2L9/P3bt2oXk5GTcfffdBtdduHABv/32G7Zt24Zt27Zh//79WLlyJQAgKysLs2fPxgMPPIAzZ85g3759mDlzJrgPMVHHxiEqIrome3t7KJVKWFlZwd3dXTy+Zs0ahISE4O233xaPffXVV/Dy8sK5c+fQs2dPAECPHj3w7rvvGrym/nweX19fvPXWW1i4cCE++eQTKJVK2NvbQyaTGbzf5cLDw3Hq1CmkpKTAy8sLAPDtt9+iT58+iI6OxqBBgwDogtCGDRtga2sLAJg3bx7Cw8OxfPlyZGVloa6uDjNnzoSPjw8AoG/fvv/i2yKi9oA9OETUYidOnMDevXthY2Mj/vj7+wPQ9Zo0GDBggNFzd+/ejXHjxqFLly6wtbXFvHnzUFBQgMrKyma//5kzZ+Dl5SWGGwAICAiAg4MDzpw5Ix7z9fUVww0AdO7cGbm5uQCA4OBgjBs3Dn379sWsWbOwfv16FBUVNf9LIKJ2iQGHiFqsvLwc06ZNQ1xcnMFPUlISRo0aJV5nbW1t8LzU1FTceuutCAoKwubNmxEbG4u1a9cC0E1Cbm3m5uYGj2UyGbRaLQBAoVBg165d+PvvvxEQEICPP/4YvXr1QkpKSqu3g4jaDgMOETWLUqmERqMxONa/f3/Ex8fD19cX3bt3N/i5PNToi42NhVarxfvvv4+hQ4eiZ8+eyMzMvOb7Xa53795IT09Henq6eCwhIQHFxcUICAho9meTyWQYPnw4Xn/9dRw/fhxKpRJbt25t9vOJqP1hwCGiZvH19UVUVBRSU1ORn58PrVaLRYsWobCwELNnz0Z0dDQuXLiAnTt3YsGCBVcNJ927d4darcbHH3+M5ORkfPfdd+LkY/33Ky8vR3h4OPLz85scugoLC0Pfvn0xd+5cHDt2DEePHsV9992H0aNHY+DAgc36XFFRUXj77bcRExODtLQ0bNmyBXl5eejdu/f1fUFE1K4w4BBRszz99NNQKBQICAhAp06dkJaWBg8PDxw6dAgajQYTJkxA3759sWTJEjg4OEAuv/I/L8HBwfjggw/wzjvvIDAwEBs3bsSKFSsMrhk2bBgWLlyIu+++G506dTKapAzoel5+//13ODo6YtSoUQgLC0PXrl3x008/Nftz2dnZISIiAlOmTEHPnj3x0ksv4f3338fkyZOb/+UQUbsjE7gWkoiIiCSGPThEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDkMOERERCQ5DDhEREQkOQw4REREJDn/B2ACb9or1w+EAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "batch_size = 256\n",
    "\n",
    "runSGD(model, input_train_processed, target_train_processed, input_test_processed, target_test_processed, n_epochs=n_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6af472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  2,  2,  ...,  2,  2, -1],\n",
      "        [ 2,  2,  2,  ...,  2,  2, -1],\n",
      "        [ 2,  2,  2,  ...,  2,  2,  0],\n",
      "        ...,\n",
      "        [ 2,  2,  2,  ...,  2,  2, -1],\n",
      "        [ 2,  2,  2,  ...,  2,  2, -1],\n",
      "        [ 2,  2,  2,  ...,  2,  2, -1]])\n",
      "Cell accuracy: 0.6818018384192096\n",
      "Agent orientation accuracy: 0.11305652826413204\n",
      "Number of agents accuracy: 0.423711855927964\n",
      "Agent location accuracy: 0.9894947473736868\n",
      "Perfectness: 0.0\n"
     ]
    }
   ],
   "source": [
    "input_test_processed = input_test_processed.to(device)\n",
    "output = model(input_test_processed)  # Model output (logits or probabilities)\n",
    "# torch.set_printoptions(profile=\"full\")\n",
    "# print(output)\n",
    "output = output.cpu() \n",
    "# Initialize a list to hold the decoded outputs for each observation\n",
    "decoded_outputs = []\n",
    "\n",
    "incorrect_agent_num=0\n",
    "incorrect_agent_location=0\n",
    "incorrect_orientation=0\n",
    "incorrect_cell=0\n",
    "not_perfect=0\n",
    "\n",
    "for i in range(output.size(0)):  # Loop over each sample in the batch\n",
    "    # Extract the blocks of the output\n",
    "    fist_NSQ = output[i]\n",
    "    # second_block = output[i, input_train_processed.size(1)-7:input_train_processed.size(1)-4]    # Next block of size 3\n",
    "    # third_block = output[i, input_train_processed.size(1)-7:]             # Last block of size 4\n",
    "\n",
    "    # Decode each block by taking the argmax (index of max value)\n",
    "    decoded_fist_NSQ = torch.round(fist_NSQ).long()  # 25 values, each between 0-10\n",
    "    # decoded_second = torch.argmax(second_block)       # Single value between 0-2\n",
    "    # decoded_third = torch.argmax(third_block)         # Single value between 0-3\n",
    "    \n",
    "    map_array = map_numbers+[11,12,13]\n",
    "    in_map_array = torch.isin(decoded_fist_NSQ, torch.tensor(range(len(map_array))))\n",
    "    # Convert values not in map_array to len(map_array)\n",
    "    adjusted_values = torch.where(in_map_array, decoded_fist_NSQ, torch.tensor(len(map_array)))\n",
    "    mapping = torch.tensor(map_array+[-1])\n",
    "    decoded_fist_NSQ = mapping[adjusted_values]\n",
    "\n",
    "    num_agents = 0\n",
    "    maybe_perfect=1\n",
    "\n",
    "    for j, val in enumerate(decoded_fist_NSQ):\n",
    "        if val >= 10 and num_agents == 0:\n",
    "            decoded_second = val -10\n",
    "            if (target_test[i, j] != 10):\n",
    "                incorrect_agent_location=1\n",
    "                maybe_perfect=0\n",
    "        if val>= 10:\n",
    "            num_agents +=1\n",
    "        if (target_test[i, j] != val and (val < 10 or target_test[i, j] < 10)):\n",
    "            # print(\"cell\", i)\n",
    "            maybe_perfect=0\n",
    "            incorrect_cell +=1\n",
    "    \n",
    "    decoded_fist_NSQ[decoded_fist_NSQ > 10] = 10\n",
    "\n",
    "    if num_agents != 1:\n",
    "        decoded_second = torch.tensor(-1, dtype=torch.long)\n",
    "        maybe_perfect=0\n",
    "        incorrect_agent_num +=1\n",
    "    \n",
    "    if (decoded_second != target_orientation_test[i]):\n",
    "        maybe_perfect=0\n",
    "        incorrect_orientation +=1\n",
    "\n",
    "    if not maybe_perfect:\n",
    "        not_perfect +=1\n",
    "\n",
    "    # Use torch.masked_select to get all matching values\n",
    "    matches = torch.masked_select(decoded_fist_NSQ, decoded_fist_NSQ >= 10)\n",
    "\n",
    "    # Ensure it's a 1D tensor for concatenation\n",
    "    decoded_second = decoded_second.unsqueeze(0)\n",
    "\n",
    "    # Combine the decoded values into a single list for this observation\n",
    "    decoded_sample = torch.cat([decoded_fist_NSQ, decoded_second])\n",
    "    \n",
    "    # Append the decoded sample to the final list\n",
    "    decoded_outputs.append(decoded_sample)\n",
    "\n",
    "acc_cell = 1.0-(incorrect_cell/(WORLD_N_SQ*output.size(0)))\n",
    "acc_orientation = 1.0-(incorrect_orientation/output.size(0))\n",
    "acc_agentloc = 1.0-(incorrect_agent_location/output.size(0))\n",
    "acc_agentnum = 1.0-(incorrect_agent_num/output.size(0))\n",
    "acc_perfect = 1.0-(not_perfect/output.size(0))\n",
    "\n",
    "# Convert to tensor if needed\n",
    "decoded_outputs = torch.stack(decoded_outputs)\n",
    "print(decoded_outputs)\n",
    "\n",
    "print (\"Cell accuracy:\", acc_cell)\n",
    "print (\"Agent orientation accuracy:\", acc_orientation)\n",
    "print (\"Number of agents accuracy:\", acc_agentnum)\n",
    "print (\"Agent location accuracy:\", acc_agentloc)\n",
    "print (\"Perfectness:\", acc_perfect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e7485691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1301:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  2,  2,  1,  1,  1,  2],\n",
      "        [ 2,  1,  3,  1,  1,  1,  2,  2],\n",
      "        [ 2,  2,  2,  3,  2,  1,  1,  2],\n",
      "        [ 2,  1, -1,  1,  1, 10,  2,  2],\n",
      "        [ 2,  3,  3,  2,  1,  2,  1,  2],\n",
      "        [ 2,  1,  2,  1,  1,  1,  7,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  6,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  6,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 1 | Target Orientation: 0 | Input Orientation: 1\n",
      "==================================================\n",
      "Sample 1302:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  6,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  6,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [2, 2, 2, 1, 2, 1, 1, 2],\n",
      "        [2, 2, 3, 2, 2, 1, 1, 2],\n",
      "        [2, 1, 2, 1, 2, 2, 2, 2],\n",
      "        [2, 1, 1, 1, 2, 4, 3, 2],\n",
      "        [2, 1, 1, 1, 2, 5, 3, 2],\n",
      "        [2, 1, 2, 1, 3, 1, 8, 2],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  6,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 2\n",
      "Decoded Orientation: -1 | Target Orientation: 0 | Input Orientation: 0\n",
      "==================================================\n",
      "Sample 1303:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  6,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  3,  2,  2,  1,  1,  2],\n",
      "        [ 2,  2,  3,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  2,  1,  2,  2],\n",
      "        [ 2,  2,  3,  2,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  2,  1,  2,  1,  2],\n",
      "        [ 2,  2,  2,  1,  3,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  6,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 0 | Target Orientation: 3 | Input Orientation: 0\n",
      "==================================================\n",
      "Sample 1304:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  6,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  3,  1,  2,  1,  2],\n",
      "        [ 2,  1,  2,  2,  2,  2,  1,  2],\n",
      "        [ 2,  2,  3,  3,  1,  1,  5,  2],\n",
      "        [ 2,  1,  2,  2,  2,  4,  6,  2],\n",
      "        [ 2,  1,  2,  1,  2, -1,  4,  2],\n",
      "        [ 2,  1,  1,  1,  2,  2,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 2\n",
      "Decoded Orientation: -1 | Target Orientation: 3 | Input Orientation: 3\n",
      "==================================================\n",
      "Sample 1305:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  2,  3,  4,  1,  2,  2],\n",
      "        [ 2,  2,  2,  1,  4, -1,  2,  2],\n",
      "        [ 2,  1,  2,  3,  1,  2, 10,  2],\n",
      "        [ 2,  1,  2,  2,  1,  3,  1,  2],\n",
      "        [ 2,  1,  2,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  2,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 0 | Target Orientation: 2 | Input Orientation: 3\n",
      "==================================================\n",
      "Sample 1306:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  6, -1,  3,  1,  2],\n",
      "        [ 2,  1, -1,  2,  2,  4,  1,  2],\n",
      "        [ 2,  2,  1,  2,  2, -1, 10,  2],\n",
      "        [ 2,  1,  2,  2,  2,  2, -1,  2],\n",
      "        [ 2,  1,  1,  1,  2,  1,  2,  2],\n",
      "        [ 2,  1,  2,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  6,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 1\n",
      "Decoded Orientation: 1 | Target Orientation: 3 | Input Orientation: 2\n",
      "==================================================\n",
      "Sample 1307:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  6,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  2,  2,  3,  3,  2,  2,  2],\n",
      "        [ 2,  2,  3,  2,  3, -1,  1,  2],\n",
      "        [ 2,  1,  3,  2,  1,  2, 10,  2],\n",
      "        [ 2,  1,  2,  1,  1,  2,  2,  2],\n",
      "        [ 2,  1,  2,  2,  1,  1,  1,  2],\n",
      "        [ 2,  2,  1,  1,  1,  2,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  6,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 1\n",
      "Decoded Orientation: 2 | Target Orientation: 0 | Input Orientation: 3\n",
      "==================================================\n",
      "Sample 1308:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  6,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  2,  3,  1, -1,  2,  1,  2],\n",
      "        [ 2,  2,  4,  3,  1,  2,  1,  2],\n",
      "        [ 2,  1,  2,  2,  1,  1,  9,  2],\n",
      "        [ 2,  1,  1,  1,  2,  3,  3,  2],\n",
      "        [ 2,  1,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  2,  1,  2,  2,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  6,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: -1 | Target Orientation: 3 | Input Orientation: 0\n",
      "==================================================\n",
      "Sample 1309:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  6,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  1,  2],\n",
      "        [ 2, -1,  3,  2,  2,  1,  3,  2],\n",
      "        [ 2,  1,  4,  2,  2,  3,  9,  2],\n",
      "        [ 2,  1,  2,  1,  1,  2,  1,  2],\n",
      "        [ 2,  1,  2,  3,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  2,  7,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: -1 | Target Orientation: 2 | Input Orientation: 3\n",
      "==================================================\n",
      "Sample 1310:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  2,  2,  2,  3,  1,  1,  2],\n",
      "        [ 2,  2,  2,  3,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2, -1,  1, 10,  2],\n",
      "        [ 2,  1,  2,  1,  1,  2,  1,  2],\n",
      "        [ 2,  1,  2,  3,  2,  1,  1,  2],\n",
      "        [ 2,  1,  2,  1,  2,  1,  7,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 0 | Target Orientation: 1 | Input Orientation: 2\n",
      "==================================================\n",
      "Sample 1311:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  2,  2,  1,  2,  3,  1,  2],\n",
      "        [ 2,  3,  2,  1,  1,  2,  1,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2, 10,  2],\n",
      "        [ 2,  3,  3,  1,  1,  2,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  2,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 1\n",
      "Decoded Orientation: 0 | Target Orientation: 2 | Input Orientation: 1\n",
      "==================================================\n",
      "Sample 1312:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  2,  2,  5,  2,  2,  1,  2],\n",
      "        [ 2,  2,  1,  2,  2,  2,  1,  2],\n",
      "        [ 2,  2,  2,  2,  1,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  1,  1, -1,  2],\n",
      "        [ 2,  1,  2,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  2,  1,  2,  1,  7,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 0 | Target Orientation: 1 | Input Orientation: 2\n",
      "==================================================\n",
      "Sample 1313:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  2,  1,  1,  1,  1,  2],\n",
      "        [ 2,  3,  4,  2,  2,  1,  2,  2],\n",
      "        [ 2,  1,  3,  2,  2,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  3,  2],\n",
      "        [ 2,  2,  2,  2,  2,  1,  1,  2],\n",
      "        [ 2,  1,  2,  2,  2,  2,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 0 | Target Orientation: 0 | Input Orientation: 1\n",
      "==================================================\n",
      "Sample 1314:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [2, 1, 1, 1, 1, 1, 1, 2],\n",
      "        [2, 1, 2, 2, 1, 3, 3, 2],\n",
      "        [2, 1, 2, 2, 1, 5, 3, 2],\n",
      "        [2, 2, 2, 1, 1, 2, 2, 2],\n",
      "        [2, 2, 3, 2, 1, 1, 1, 2],\n",
      "        [2, 3, 3, 1, 2, 1, 8, 2],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 2\n",
      "Decoded Orientation: -1 | Target Orientation: 0 | Input Orientation: 0\n",
      "==================================================\n",
      "Sample 1315:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [2, 1, 2, 2, 1, 2, 1, 2],\n",
      "        [2, 3, 2, 2, 2, 2, 4, 2],\n",
      "        [2, 2, 1, 2, 1, 4, 4, 2],\n",
      "        [2, 2, 2, 1, 1, 2, 4, 2],\n",
      "        [2, 2, 2, 1, 1, 1, 1, 2],\n",
      "        [2, 2, 3, 2, 1, 1, 8, 2],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1,  6,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 2\n",
      "Decoded Orientation: -1 | Target Orientation: 0 | Input Orientation: 0\n",
      "==================================================\n",
      "Sample 1316:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1,  6,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  2,  2,  2,  1,  2,  2],\n",
      "        [ 2,  2,  1,  3,  3,  2,  4,  2],\n",
      "        [ 2,  1,  2,  3,  1,  1,  7,  2],\n",
      "        [ 2,  1,  2,  1,  1,  3,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, -1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  6,  1,  1,  1,  6,  1,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: -1 | Target Orientation: 3 | Input Orientation: 0\n",
      "==================================================\n",
      "Sample 1317:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  6,  1,  1,  1,  6,  1,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  2,  3,  1,  2],\n",
      "        [ 2,  2,  3,  1,  3, -1,  6,  2],\n",
      "        [ 2,  2,  3,  2,  2,  4,  5,  2],\n",
      "        [ 2,  2,  1,  2,  1,  2,  3,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  2,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  6,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  6,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 2\n",
      "Decoded Orientation: -1 | Target Orientation: 3 | Input Orientation: 3\n",
      "==================================================\n",
      "Sample 1318:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  6,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  6,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  2,  2,  2,  3,  5,  2],\n",
      "        [ 2,  3,  2,  2,  2,  4,  7,  2],\n",
      "        [ 2,  2,  2,  1,  2,  1, -1,  2],\n",
      "        [ 2,  1,  2,  2,  1,  1,  1,  2],\n",
      "        [ 2,  1,  3,  1,  2,  1, -1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  6,  1, 10,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: -1 | Target Orientation: 2 | Input Orientation: 3\n",
      "==================================================\n",
      "Sample 1319:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  6,  1, 10,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [2, 2, 1, 3, 2, 2, 4, 2],\n",
      "        [2, 3, 2, 2, 2, 3, 5, 2],\n",
      "        [2, 1, 3, 3, 1, 1, 4, 2],\n",
      "        [2, 3, 2, 2, 1, 2, 1, 2],\n",
      "        [2, 2, 1, 1, 1, 1, 1, 2],\n",
      "        [2, 1, 1, 1, 1, 1, 8, 2],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 2\n",
      "Decoded Orientation: -1 | Target Orientation: 2 | Input Orientation: 2\n",
      "==================================================\n",
      "Sample 1320:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  3,  1,  2,  1,  1,  2],\n",
      "        [ 2,  2,  3,  3,  2, 10,  1,  2],\n",
      "        [ 2,  2,  2,  3,  2,  1,  1,  2],\n",
      "        [ 2,  2,  2,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, -1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  6,  1,  6,  6, 10,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 1 | Target Orientation: 1 | Input Orientation: 2\n",
      "==================================================\n",
      "Sample 1321:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  6,  1,  6,  6, 10,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  2,  3,  2,  2,  4,  1,  2],\n",
      "        [ 2,  1,  4,  2,  3,  8,  1,  2],\n",
      "        [ 2,  2,  2,  3,  2,  1,  1,  2],\n",
      "        [ 2,  2,  2,  2,  1,  1,  1,  2],\n",
      "        [ 2,  1, -1,  1,  1,  2,  1,  2],\n",
      "        [ 2,  2,  1,  1, -1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  6,  6,  1,  1,  1,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: -1 | Target Orientation: 0 | Input Orientation: 1\n",
      "==================================================\n",
      "Sample 1322:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  6,  6,  1,  1,  1,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  2, -1,  2,  2,  1,  2],\n",
      "        [ 2,  3,  5,  1,  3,  7,  1,  2],\n",
      "        [ 2,  2,  3,  2,  2,  1,  2,  2],\n",
      "        [ 2,  2,  3,  2,  2,  1,  1,  2],\n",
      "        [ 2,  2,  3,  1,  1,  1,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: -1 | Target Orientation: 3 | Input Orientation: 0\n",
      "==================================================\n",
      "Sample 1323:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  2,  2,  1,  4,  1,  2],\n",
      "        [ 2,  3, -1,  2,  4,  4,  2,  2],\n",
      "        [ 2,  2,  3,  2,  2,  4,  1,  2],\n",
      "        [ 2,  2,  1,  3,  1,  3,  1,  2],\n",
      "        [ 2,  1,  2,  1,  2,  2,  1,  2],\n",
      "        [ 2,  2,  1,  1, -1,  2,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  6,  1,  1,  2],\n",
      "        [ 2,  6,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 2\n",
      "Decoded Orientation: -1 | Target Orientation: 3 | Input Orientation: 3\n",
      "==================================================\n",
      "Sample 1324:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  6,  1,  1,  2],\n",
      "        [ 2,  6,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  1,  2],\n",
      "        [ 2,  2,  1,  2,  2,  1,  1,  2],\n",
      "        [ 2,  2,  1,  4,  2,  1,  1,  2],\n",
      "        [ 2,  1,  3,  2,  1,  3,  1,  2],\n",
      "        [ 2,  2,  2,  1,  2,  2,  2,  2],\n",
      "        [ 2,  2,  1,  2,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  6,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 3 | Target Orientation: 2 | Input Orientation: 3\n",
      "==================================================\n",
      "Sample 1325:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  6,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  2, 10,  1,  2],\n",
      "        [ 2,  2,  2,  1,  3, -1,  1,  2],\n",
      "        [ 2,  1,  1,  3,  3,  1,  1,  2],\n",
      "        [ 2,  1,  2,  1,  2,  2,  1,  2],\n",
      "        [ 2,  2,  1,  3,  2,  1,  2,  2],\n",
      "        [ 2,  2,  1,  2,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  6, 10,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  6,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 1\n",
      "Decoded Orientation: 0 | Target Orientation: 3 | Input Orientation: 2\n",
      "==================================================\n",
      "Sample 1326:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  6, 10,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  6,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  6,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [2, 1, 1, 2, 4, 7, 2, 2],\n",
      "        [2, 1, 2, 2, 2, 3, 1, 2],\n",
      "        [2, 1, 1, 1, 3, 2, 3, 2],\n",
      "        [2, 2, 2, 1, 2, 1, 1, 2],\n",
      "        [2, 2, 1, 3, 2, 2, 1, 2],\n",
      "        [2, 3, 1, 2, 1, 1, 8, 2],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  6,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  6,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 2\n",
      "Decoded Orientation: -1 | Target Orientation: 3 | Input Orientation: 3\n",
      "==================================================\n",
      "Sample 1327:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  6,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  6,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  3,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  2,  2,  3,  1,  2,  2],\n",
      "        [ 2,  1,  1,  1,  2,  1,  2,  2],\n",
      "        [ 2, -1,  2,  2,  3,  2,  2,  2],\n",
      "        [ 2,  1,  2,  1,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  6,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  6,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  6,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 3 | Target Orientation: 2 | Input Orientation: 3\n",
      "==================================================\n",
      "Sample 1328:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  6,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  6,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  6,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  6,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  3,  1,  2,  1, 10,  1,  2],\n",
      "        [ 2,  3,  2,  2,  1,  2,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  3,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1,  2,  2,  2],\n",
      "        [ 2,  1,  3,  2,  2,  1,  2,  2],\n",
      "        [ 2,  1,  2,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  6,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  6,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 1\n",
      "Decoded Orientation: 2 | Target Orientation: 3 | Input Orientation: 2\n",
      "==================================================\n",
      "Sample 1329:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  6,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  6,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  2,  3,  1, -1, -1,  1,  2],\n",
      "        [ 2,  4,  2,  2, -1,  3,  2,  2],\n",
      "        [ 2, -1,  1,  1,  1,  3,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  3,  2,  2],\n",
      "        [ 2,  1,  2,  2,  2,  2,  1,  2],\n",
      "        [ 2,  1,  1,  2,  2,  2,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  6,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  6,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  6,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: -1 | Target Orientation: 2 | Input Orientation: 3\n",
      "==================================================\n",
      "Sample 1330:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  6,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  6,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  6,  1,  2],\n",
      "        [ 2,  1,  1,  1,  6,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  2,  2,  2,  2, 10,  1,  2],\n",
      "        [ 2,  1,  2,  2,  3,  2,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  2,  2,  3,  2],\n",
      "        [ 2,  1,  2,  1,  2,  3,  2,  2],\n",
      "        [ 2,  1,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  6,  1,  1,  1,  1,  6,  2],\n",
      "        [ 2,  1,  1,  1,  1,  6,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  6,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 0 | Target Orientation: 1 | Input Orientation: 2\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "interval_min=1300\n",
    "interval_size=30\n",
    "for i in range(interval_min,interval_min+min(interval_size,decoded_outputs.size(0))):\n",
    "    # Print the input, decoded grid, and target test grid side by side\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    \n",
    "    # Input grid (first 25 values reshaped into 5x5)\n",
    "    print(\"Input Grid:\")\n",
    "    print(input_test[i, :WORLD_N_SQ].reshape(WORLD_N, WORLD_N).int())\n",
    "    \n",
    "    # Decoded grid (first 25 values reshaped into 5x5)\n",
    "    print(\"Decoded Grid:\")\n",
    "    print(decoded_outputs[i, :WORLD_N_SQ].reshape(WORLD_N, WORLD_N).int())\n",
    "    \n",
    "    # Target test grid (first 25 values reshaped into 5x5)\n",
    "    print(\"Target Grid:\")\n",
    "    print(target_test[i, :WORLD_N_SQ].reshape(WORLD_N, WORLD_N).int())\n",
    "    \n",
    "    # Action and Orientation for input, decoded, and target test\n",
    "    print(f\"Input Action: {input_test[i, WORLD_N_SQ].item()}\")\n",
    "    print(f\"Decoded Orientation: {decoded_outputs[i, WORLD_N_SQ].item()} | Target Orientation: {target_orientation_test[i].item()} | Input Orientation: {input_orientation_test[i].item()}\")\n",
    "    \n",
    "    print(\"=\" * 50)  # Separator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f2204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mg10 (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
